{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHN9RN-7vAFp",
        "outputId": "6bb08436-7bde-49d9-9292-c37f0802ef97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzT0rEY9vE0u",
        "outputId": "da4f73d9-d1d3-4a94-e8d1-65093f8a5a5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# Instalar las bibliotecas necesarias\n",
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Lui8sH2vGIJ"
      },
      "outputs": [],
      "source": [
        "# Importar bibliotecas\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFjAqgZivHe8"
      },
      "outputs": [],
      "source": [
        "# Función para leer párrafos de los archivos\n",
        "def leer_parrafos(archivo):\n",
        "    with open(archivo, 'r', encoding='utf-8') as f:\n",
        "        contenido = f.read()\n",
        "    parrafos = [p.strip() for p in contenido.split('*') if p.strip()]\n",
        "    return parrafos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2S_8KctvJQa"
      },
      "outputs": [],
      "source": [
        "# Ruta al dataset\n",
        "ruta = '/content/drive/MyDrive/dataset_escuela'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kvgn8MC_vMQY"
      },
      "outputs": [],
      "source": [
        "# Leer los archivos y asignar etiquetas\n",
        "parrafos_estudiantes = leer_parrafos(ruta + '/normal.txt')\n",
        "etiquetas_estudiantes = [0] * len(parrafos_estudiantes)\n",
        "\n",
        "parrafos_docentes = leer_parrafos(ruta + '/ofensivo.txt')\n",
        "etiquetas_docentes = [1] * len(parrafos_docentes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUenBNV0vNsF"
      },
      "outputs": [],
      "source": [
        "# Combinar todos los párrafos y etiquetas\n",
        "todos_parrafos = parrafos_estudiantes + parrafos_docentes\n",
        "todas_etiquetas = etiquetas_estudiantes + etiquetas_docentes\n",
        "\n",
        "# Dividir en conjuntos de entrenamiento y prueba\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    todos_parrafos, todas_etiquetas, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnaNXLvAvQM3"
      },
      "outputs": [],
      "source": [
        "# Definir la clase personalizada para el dataset\n",
        "class TextoDataset(Dataset):\n",
        "    def __init__(self, textos, etiquetas, tokenizer, max_length=512):\n",
        "        self.textos = textos\n",
        "        self.etiquetas = etiquetas\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.textos)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        texto = self.textos[idx]\n",
        "        etiqueta = self.etiquetas[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            texto, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(etiqueta, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS-4kIN6vRta"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Cargar el tokenizador de BERT en español uncased\n",
        "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "\n",
        "# Crear los datasets de entrenamiento y prueba\n",
        "train_dataset = TextoDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = TextoDataset(test_texts, test_labels, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY9UAxK7vVPz",
        "outputId": "2238b53b-acad-4825-b2fd-1bcede0fde27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Crear DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Cargar el modelo de BERT en español uncased para clasificación\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'dccuchile/bert-base-spanish-wwm-uncased',\n",
        "    num_labels=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDOU00Hs0IQJ",
        "outputId": "208c477f-3d83-4b18-b156-02b03f3dc438"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mover el modelo a la GPU si está disponible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah-BDEmPyx6D"
      },
      "outputs": [],
      "source": [
        "# Definir el optimizador y la función de pérdida\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTSbQ2eVy3Dt"
      },
      "outputs": [],
      "source": [
        "# Función de entrenamiento manual\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=\"Entrenando\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device) # Move labels to the device\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYcaN2owvZs1"
      },
      "outputs": [],
      "source": [
        "# Función de evaluación manual\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluando\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device) # Move labels to the device\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    return total_loss / len(test_loader), accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FspsW8gwzvg3",
        "outputId": "6b746af9-d232-4149-e1a9-9a8c3a30ef38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:35<00:00,  2.70s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3010\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.58s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2966\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 3/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:34<00:00,  2.65s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2630\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 4/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2679\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 5/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.60s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3351\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 6/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2524\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 7/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.60s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3064\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 8/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2646\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 9/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2741\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 10/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2897\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 11/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2319\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 12/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2723\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 13/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2638\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 14/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.60s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3401\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 15/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3022\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 16/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3040\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 17/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3540\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 18/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:34<00:00,  2.62s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3358\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 19/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2882\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 20/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:34<00:00,  2.62s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3270\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 21/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3129\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 22/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3100\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 23/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:34<00:00,  2.62s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2828\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 24/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:34<00:00,  2.62s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3186\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 25/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2832\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 26/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2613\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 27/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.62s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3075\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 28/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:33<00:00,  2.61s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.3340\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 29/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:34<00:00,  2.62s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2781\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n",
            "\n",
            "Epoch 30/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entrenando: 100%|██████████| 13/13 [00:34<00:00,  2.63s/it]\n",
            "Evaluando: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pérdida de entrenamiento: 0.2912\n",
            "Pérdida de prueba: 0.1917, Precisión en prueba: 0.9490\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Entrenamiento y evaluación manual\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
        "    print(f\"Pérdida de entrenamiento: {train_loss:.4f}\")\n",
        "    print(f\"Pérdida de prueba: {test_loss:.4f}, Precisión en prueba: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Vu8dg_03Vo"
      },
      "outputs": [],
      "source": [
        "# Definir las clases\n",
        "CLASES = {\n",
        "    0: \"Normal\",\n",
        "    1: \"Ofensivo\"\n",
        "}\n",
        "def inferir_clase(texto, model_path, tokenizer_path, max_length=512):\n",
        "    \"\"\"\n",
        "    Función para inferir la clase de un texto dado usando un modelo BERT entrenado.\n",
        "\n",
        "    Args:\n",
        "        texto (str): El texto a clasificar.\n",
        "        model_path (str): Ruta al directorio donde se guardó el modelo entrenado.\n",
        "        tokenizer_path (str): Ruta al directorio donde se guardó el tokenizador.\n",
        "        max_length (int): Longitud máxima de la secuencia para el tokenizador.\n",
        "\n",
        "    Returns:\n",
        "        str: La clase predicha del texto.\n",
        "    \"\"\"\n",
        "    # Cargar el tokenizador y el modelo\n",
        "    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "    # Mover el modelo a la GPU si está disponible\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    encoding = tokenizer(\n",
        "        texto,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Mover los tensores al dispositivo adecuado\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # Realizar la inferencia\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        prediccion = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Devolver la clase predicha\n",
        "    return CLASES[prediccion]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp4xeh3G0YjD",
        "outputId": "716da3f5-5d0c-46f3-e8bf-883c83bff7c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/dataset_escuela/bert_tokenizer_manual_final/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/dataset_escuela/bert_tokenizer_manual_final/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/dataset_escuela/bert_tokenizer_manual_final/vocab.txt',\n",
              " '/content/drive/MyDrive/dataset_escuela/bert_tokenizer_manual_final/added_tokens.json')"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Guardar el modelo entrenado (opcional)\n",
        "model.save_pretrained('/content/drive/MyDrive/dataset_escuela/bert_model_manual_final')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/dataset_escuela/bert_tokenizer_manual_final')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRnyJ76ZM9d1",
        "outputId": "e4e86259-456d-447a-bd71-6c614954b7b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path = '/content/drive/MyDrive/dataset_escuela/bert_model_manual_final'\n",
        "tokenizer_path = '/content/drive/MyDrive/dataset_escuela/bert_tokenizer_manual_final'\n",
        "# Cargar el tokenizador y el modelo\n",
        "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3ZPvYjr1Dvt",
        "outputId": "27713637-0043-4a6c-87c3-65990e5562f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El texto pertenece a la clase: OFENSIVO\n"
          ]
        }
      ],
      "source": [
        "texto_ejemplo = \"Me gusta que se interesen por los temas\"\n",
        "\n",
        "clase_predicha = inferir_clase(texto_ejemplo, model_path, tokenizer_path)\n",
        "print(f\"El texto pertenece a la clase: {clase_predicha}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YCQCZS0Qmbc"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/dataset_escuela/bert_model_manual_final'\n",
        "tokenizer_path = '/content/drive/MyDrive/dataset_escuela/bert_tokenizer_manual_final'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0qrxaCMHbe_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Definir las clases\n",
        "CLASES = {\n",
        "    0: \"NORMAL\",\n",
        "    1: \"OFENSIVO\"\n",
        "}\n",
        "\n",
        "def inferir_clase(texto, model_path, tokenizer_path, max_length=512):\n",
        "    \"\"\"\n",
        "    Función para inferir la clase de un texto dado usando un modelo BERT entrenado.\n",
        "\n",
        "    Args:\n",
        "        texto (str): El texto a clasificar.\n",
        "        model_path (str): Ruta al directorio donde se guardó el modelo entrenado.\n",
        "        tokenizer_path (str): Ruta al directorio donde se guardó el tokenizador.\n",
        "        max_length (int): Longitud máxima de la secuencia para el tokenizador.\n",
        "\n",
        "    Returns:\n",
        "        str: La clase predicha del texto (\"NORMAL\" o \"OFENSIVO\").\n",
        "    \"\"\"\n",
        "    # Cargar el tokenizador y el modelo\n",
        "    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "    # Mover el modelo a la GPU si está disponible\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    encoding = tokenizer(\n",
        "        texto,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Mover los tensores al dispositivo adecuado\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # Realizar la inferencia\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # CORRECCIÓN: Usar índice [0] para obtener el primer (y único) elemento del batch\n",
        "        prediccion = torch.argmax(logits, dim=1)[0].item()\n",
        "\n",
        "    # Devolver la clase predicha\n",
        "    return CLASES[prediccion]\n",
        "\n",
        "# Función alternativa más robusta\n",
        "def inferir_clase_con_probabilidades(texto, model_path, tokenizer_path, max_length=512):\n",
        "    \"\"\"\n",
        "    Versión extendida que también devuelve las probabilidades de cada clase.\n",
        "    \"\"\"\n",
        "    # Cargar el tokenizador y el modelo\n",
        "    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "    # Mover el modelo a la GPU si está disponible\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    encoding = tokenizer(\n",
        "        texto,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Mover los tensores al dispositivo adecuado\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # Realizar la inferencia\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Aplicar softmax para obtener probabilidades\n",
        "        probabilidades = torch.softmax(logits, dim=1)\n",
        "\n",
        "        # Obtener la predicción\n",
        "        prediccion = torch.argmax(logits, dim=1)[0].item()\n",
        "\n",
        "        # Obtener las probabilidades individuales\n",
        "        prob_normal = probabilidades[0][0].item()\n",
        "        prob_ofensivo = probabilidades[0][1].item()\n",
        "\n",
        "    return {\n",
        "        'clase': CLASES[prediccion],\n",
        "        'probabilidad_normal': prob_normal,\n",
        "        'probabilidad_ofensivo': prob_ofensivo,\n",
        "        'confianza': max(prob_normal, prob_ofensivo)\n",
        "    }\n",
        "\n",
        "# Función de prueba corregida\n",
        "def probar_modelo_completo_corregido(model_path, tokenizer_path):\n",
        "    \"\"\"Función principal para probar el modelo con múltiples casos - VERSIÓN CORREGIDA\"\"\"\n",
        "\n",
        "    # Configuración de rutas\n",
        "    print(\"=\"*70)\n",
        "    print(\"EVALUACIÓN COMPLETA DEL MODELO BERT CLASIFICADOR\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. CASOS BÁSICOS ESPERADOS\n",
        "    print(\"\\n1. CASOS BÁSICOS ESPERADOS\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    casos_normales_basicos = [\n",
        "        \"Hoy vamos a ver anatomía humana\",\n",
        "        \"Abran el libro en la página veinte\",\n",
        "        \"María, puedes leer el siguiente párrafo\",\n",
        "        \"En el zoológico vimos un mono negro\",\n",
        "        \"Hay una chica que lava platos en la cafetería\",\n",
        "        \"Los estudiantes de diferentes culturas enriquecen la clase\",\n",
        "        \"Vamos a estudiar sobre la diversidad étnica\"\n",
        "    ]\n",
        "\n",
        "    casos_ofensivos_basicos = [\n",
        "        \"Cállate mono negro, no interrumpas\",\n",
        "        \"Esa chica solo sirve para lavar platos\",\n",
        "        \"Las mujeres deberían estar en la cocina\",\n",
        "        \"Tu color de piel no te permite ser inteligente\",\n",
        "        \"Los homosexuales no deberían estar aquí\"\n",
        "    ]\n",
        "\n",
        "    print(\"CASOS NORMALES:\")\n",
        "    for texto in casos_normales_basicos:\n",
        "        try:\n",
        "            prediccion = inferir_clase(texto, model_path, tokenizer_path)\n",
        "            estado = \"✓\" if prediccion == \"NORMAL\" else \"✗\"\n",
        "            print(f\"{estado} '{texto}' → {prediccion}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR en '{texto}': {str(e)}\")\n",
        "\n",
        "    print(\"\\nCASOS OFENSIVOS:\")\n",
        "    for texto in casos_ofensivos_basicos:\n",
        "        try:\n",
        "            prediccion = inferir_clase(texto, model_path, tokenizer_path)\n",
        "            estado = \"✓\" if prediccion == \"OFENSIVO\" else \"✗\"\n",
        "            print(f\"{estado} '{texto}' → {prediccion}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR en '{texto}': {str(e)}\")\n",
        "\n",
        "    # 2. CASOS ENGAÑOSOS - PALABRAS AMBIGUAS\n",
        "    print(\"\\n\\n2. CASOS ENGAÑOSOS - PALABRAS AMBIGUAS\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    casos_engañosos = [\n",
        "        # Casos con palabras que podrían ser ofensivas pero en contexto educativo\n",
        "        (\"En el laboratorio vamos a ver células negras y blancas\", \"NORMAL\"),\n",
        "        (\"El personaje negro de la novela representa la justicia\", \"NORMAL\"),\n",
        "        (\"Los puntos negros en el mapa indican las ciudades\", \"NORMAL\"),\n",
        "        (\"La lista negra de palabras prohibidas en el examen\", \"NORMAL\"),\n",
        "        (\"En química vemos la reacción del carbón negro\", \"NORMAL\"),\n",
        "\n",
        "        # Casos con referencias a género en contexto académico\n",
        "        (\"Las mujeres científicas han hecho grandes aportes\", \"NORMAL\"),\n",
        "        (\"En la historia vemos cómo los hombres y mujeres trabajaban\", \"NORMAL\"),\n",
        "        (\"La protagonista femenina del libro es muy valiente\", \"NORMAL\"),\n",
        "        (\"Los roles de género han cambiado a lo largo de la historia\", \"NORMAL\"),\n",
        "\n",
        "        # Casos con referencias étnicas en contexto educativo\n",
        "        (\"Los pueblos indígenas tenían conocimientos avanzados\", \"NORMAL\"),\n",
        "        (\"La cultura africana es muy rica en tradiciones\", \"NORMAL\"),\n",
        "        (\"Los inmigrantes asiáticos trajeron nuevas tecnologías\", \"NORMAL\"),\n",
        "        (\"La población gitana tiene una cultura nómada interesante\", \"NORMAL\"),\n",
        "\n",
        "        # Casos sutilmente ofensivos (más difíciles de detectar)\n",
        "        (\"Algunos estudiantes simplemente no tienen capacidad para esto\", \"OFENSIVO\"),\n",
        "        (\"Ciertos alumnos traen problemas de su ambiente familiar\", \"OFENSIVO\"),\n",
        "        (\"Esos niños de barrio no entienden estas cosas finas\", \"OFENSIVO\"),\n",
        "        (\"Las niñas son naturalmente peores en matemáticas\", \"OFENSIVO\"),\n",
        "        (\"Tu tipo de gente no suele destacar en ciencias\", \"OFENSIVO\"),\n",
        "    ]\n",
        "\n",
        "    errores = 0\n",
        "    for texto, esperado in casos_engañosos:\n",
        "        try:\n",
        "            prediccion = inferir_clase(texto, model_path, tokenizer_path)\n",
        "            estado = \"✓\" if prediccion == esperado else \"✗\"\n",
        "            if prediccion != esperado:\n",
        "                errores += 1\n",
        "            print(f\"{estado} '{texto}' → {prediccion} (esperado: {esperado})\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR en '{texto}': {str(e)}\")\n",
        "            errores += 1\n",
        "\n",
        "    # 3. CASOS LÍMITE - MUY SUTILES\n",
        "    print(\"\\n\\n3. CASOS LÍMITE - MUY SUTILES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    casos_limite = [\n",
        "        # Borderline - podrían ir en cualquier dirección\n",
        "        (\"No todos los estudiantes tienen la misma capacidad\", \"NORMAL\"),\n",
        "        (\"Algunos alumnos necesitan más apoyo que otros\", \"NORMAL\"),\n",
        "        (\"Tu rendimiento no es el esperado para alguien como tú\", \"OFENSIVO\"),\n",
        "        (\"Las chicas suelen ser más ordenadas que los chicos\", \"OFENSIVO\"),\n",
        "        (\"Los chicos son naturalmente más agresivos\", \"OFENSIVO\"),\n",
        "        (\"Tu familia probablemente no valora la educación\", \"OFENSIVO\"),\n",
        "        (\"Estudiantes de tu condición social suelen abandonar\", \"OFENSIVO\"),\n",
        "        (\"No esperaba mucho de ti considerando tu origen\", \"OFENSIVO\"),\n",
        "    ]\n",
        "\n",
        "    for texto, esperado in casos_limite:\n",
        "        try:\n",
        "            prediccion = inferir_clase(texto, model_path, tokenizer_path)\n",
        "            estado = \"✓\" if prediccion == esperado else \"✗\"\n",
        "            if prediccion != esperado:\n",
        "                errores += 1\n",
        "            print(f\"{estado} '{texto}' → {prediccion} (esperado: {esperado})\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR en '{texto}': {str(e)}\")\n",
        "            errores += 1\n",
        "\n",
        "    # 4. CASOS CON NEGACIONES\n",
        "    print(\"\\n\\n4. CASOS CON NEGACIONES\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    casos_negacion = [\n",
        "        (\"No vamos a discriminar a ningún estudiante\", \"NORMAL\"),\n",
        "        (\"No toleraremos insultos en esta clase\", \"NORMAL\"),\n",
        "        (\"No es cierto que las mujeres sean peores en matemáticas\", \"NORMAL\"),\n",
        "        (\"No deberías comportarte como un mono\", \"OFENSIVO\"),\n",
        "        (\"No me extraña que falles, eres de barrio\", \"OFENSIVO\"),\n",
        "        (\"No esperaba nada bueno de tu tipo\", \"OFENSIVO\"),\n",
        "    ]\n",
        "\n",
        "    for texto, esperado in casos_negacion:\n",
        "        try:\n",
        "            prediccion = inferir_clase(texto, model_path, tokenizer_path)\n",
        "            estado = \"✓\" if prediccion == esperado else \"✗\"\n",
        "            if prediccion != esperado:\n",
        "                errores += 1\n",
        "            print(f\"{estado} '{texto}' → {prediccion} (esperado: {esperado})\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR en '{texto}': {str(e)}\")\n",
        "            errores += 1\n",
        "\n",
        "    # 5. CASOS CON CONTEXTO ACADÉMICO ESPECÍFICO\n",
        "    print(\"\\n\\n5. CONTEXTO ACADÉMICO ESPECÍFICO\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    casos_academicos = [\n",
        "        # Anatomía y biología\n",
        "        (\"En anatomía estudiamos las diferencias entre sexos\", \"NORMAL\"),\n",
        "        (\"Los caracteres sexuales secundarios aparecen en la pubertad\", \"NORMAL\"),\n",
        "        (\"Las diferencias raciales son principalmente superficiales\", \"NORMAL\"),\n",
        "\n",
        "        # Historia y sociología\n",
        "        (\"La esclavitud fue una época oscura de la humanidad\", \"NORMAL\"),\n",
        "        (\"Los conflictos étnicos han marcado la historia\", \"NORMAL\"),\n",
        "        (\"El machismo ha sido un problema histórico\", \"NORMAL\"),\n",
        "\n",
        "        # Literatura\n",
        "        (\"En esta obra el autor usa términos despectivos de la época\", \"NORMAL\"),\n",
        "        (\"El personaje expresa ideas racistas que debemos analizar\", \"NORMAL\"),\n",
        "        (\"La novela critica los estereotipos de género\", \"NORMAL\"),\n",
        "    ]\n",
        "\n",
        "    for texto in casos_academicos:\n",
        "        try:\n",
        "            prediccion = inferir_clase(texto, model_path, tokenizer_path)\n",
        "            estado = \"✓\" if prediccion == \"NORMAL\" else \"✗\"\n",
        "            if prediccion != \"NORMAL\":\n",
        "                errores += 1\n",
        "            print(f\"{estado} '{texto}' → {prediccion}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR en '{texto}': {str(e)}\")\n",
        "            errores += 1\n",
        "\n",
        "    # 6. ANÁLISIS DE MÉTRICAS\n",
        "    print(\"\\n\\n6. RESUMEN DE PRUEBAS\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Calcular estadísticas básicas\n",
        "    todos_los_casos = (\n",
        "        [(texto, \"NORMAL\") for texto in casos_normales_basicos] +\n",
        "        [(texto, \"OFENSIVO\") for texto in casos_ofensivos_basicos] +\n",
        "        casos_engañosos + casos_limite + casos_negacion +\n",
        "        [(texto, \"NORMAL\") for texto in casos_academicos]\n",
        "    )\n",
        "\n",
        "    correctos = 0\n",
        "    total = len(todos_los_casos)\n",
        "\n",
        "    for texto, esperado in todos_los_casos:\n",
        "        try:\n",
        "            prediccion = inferir_clase(texto, model_path, tokenizer_path)\n",
        "            if prediccion == esperado:\n",
        "                correctos += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando '{texto}': {str(e)}\")\n",
        "\n",
        "    precision = (correctos / total) * 100 if total > 0 else 0\n",
        "    print(f\"Precisión general: {correctos}/{total} ({precision:.1f}%)\")\n",
        "    print(f\"Errores totales: {errores}\")\n",
        "\n",
        "    if precision >= 90:\n",
        "        print(\"🎉 EXCELENTE: El modelo tiene muy buen rendimiento\")\n",
        "    elif precision >= 80:\n",
        "        print(\"✅ BUENO: El modelo funciona bien, pero se puede mejorar\")\n",
        "    elif precision >= 70:\n",
        "        print(\"⚠️ REGULAR: El modelo necesita más entrenamiento\")\n",
        "    else:\n",
        "        print(\"❌ MALO: El modelo necesita revisión completa\")\n",
        "\n",
        "    return precision\n",
        "\n",
        "# Prueba individual con probabilidades\n",
        "def probar_texto_individual(texto, model_path, tokenizer_path):\n",
        "    \"\"\"Prueba un texto individual mostrando probabilidades detalladas\"\"\"\n",
        "    print(f\"\\n🔍 ANÁLISIS DETALLADO:\")\n",
        "    print(f\"Texto: '{texto}'\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        resultado = inferir_clase_con_probabilidades(texto, model_path, tokenizer_path)\n",
        "\n",
        "        print(f\"Predicción: {resultado['clase']}\")\n",
        "        print(f\"Confianza: {resultado['confianza']:.3f}\")\n",
        "        print(f\"Probabilidad NORMAL: {resultado['probabilidad_normal']:.3f}\")\n",
        "        print(f\"Probabilidad OFENSIVO: {resultado['probabilidad_ofensivo']:.3f}\")\n",
        "\n",
        "        if resultado['confianza'] < 0.7:\n",
        "            print(\"⚠️ ADVERTENCIA: Baja confianza en la predicción\")\n",
        "        elif resultado['confianza'] > 0.9:\n",
        "            print(\"✅ Alta confianza en la predicción\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-iUV6GxHdBl",
        "outputId": "e523939a-01c9-4e2a-9da9-8c5ace1aecd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EVALUACIÓN COMPLETA DEL MODELO BERT CLASIFICADOR\n",
            "======================================================================\n",
            "\n",
            "1. CASOS BÁSICOS ESPERADOS\n",
            "----------------------------------------\n",
            "CASOS NORMALES:\n",
            "✓ 'Hoy vamos a ver anatomía humana' → NORMAL\n",
            "✓ 'Abran el libro en la página veinte' → NORMAL\n",
            "✓ 'María, puedes leer el siguiente párrafo' → NORMAL\n",
            "✓ 'En el zoológico vimos un mono negro' → NORMAL\n",
            "✗ 'Hay una chica que lava platos en la cafetería' → OFENSIVO\n",
            "✓ 'Los estudiantes de diferentes culturas enriquecen la clase' → NORMAL\n",
            "✓ 'Vamos a estudiar sobre la diversidad étnica' → NORMAL\n",
            "\n",
            "CASOS OFENSIVOS:\n",
            "✓ 'Cállate mono negro, no interrumpas' → OFENSIVO\n",
            "✓ 'Esa chica solo sirve para lavar platos' → OFENSIVO\n",
            "✓ 'Las mujeres deberían estar en la cocina' → OFENSIVO\n",
            "✓ 'Tu color de piel no te permite ser inteligente' → OFENSIVO\n",
            "✓ 'Los homosexuales no deberían estar aquí' → OFENSIVO\n",
            "\n",
            "\n",
            "2. CASOS ENGAÑOSOS - PALABRAS AMBIGUAS\n",
            "--------------------------------------------------\n",
            "✓ 'En el laboratorio vamos a ver células negras y blancas' → NORMAL (esperado: NORMAL)\n",
            "✓ 'El personaje negro de la novela representa la justicia' → NORMAL (esperado: NORMAL)\n",
            "✓ 'Los puntos negros en el mapa indican las ciudades' → NORMAL (esperado: NORMAL)\n",
            "✓ 'La lista negra de palabras prohibidas en el examen' → NORMAL (esperado: NORMAL)\n",
            "✓ 'En química vemos la reacción del carbón negro' → NORMAL (esperado: NORMAL)\n",
            "✓ 'Las mujeres científicas han hecho grandes aportes' → NORMAL (esperado: NORMAL)\n",
            "✓ 'En la historia vemos cómo los hombres y mujeres trabajaban' → NORMAL (esperado: NORMAL)\n",
            "✓ 'La protagonista femenina del libro es muy valiente' → NORMAL (esperado: NORMAL)\n",
            "✓ 'Los roles de género han cambiado a lo largo de la historia' → NORMAL (esperado: NORMAL)\n",
            "✓ 'Los pueblos indígenas tenían conocimientos avanzados' → NORMAL (esperado: NORMAL)\n",
            "✓ 'La cultura africana es muy rica en tradiciones' → NORMAL (esperado: NORMAL)\n",
            "✗ 'Los inmigrantes asiáticos trajeron nuevas tecnologías' → OFENSIVO (esperado: NORMAL)\n",
            "✓ 'La población gitana tiene una cultura nómada interesante' → NORMAL (esperado: NORMAL)\n",
            "✗ 'Algunos estudiantes simplemente no tienen capacidad para esto' → NORMAL (esperado: OFENSIVO)\n",
            "✓ 'Ciertos alumnos traen problemas de su ambiente familiar' → OFENSIVO (esperado: OFENSIVO)\n",
            "✓ 'Esos niños de barrio no entienden estas cosas finas' → OFENSIVO (esperado: OFENSIVO)\n",
            "✗ 'Las niñas son naturalmente peores en matemáticas' → NORMAL (esperado: OFENSIVO)\n",
            "✓ 'Tu tipo de gente no suele destacar en ciencias' → OFENSIVO (esperado: OFENSIVO)\n",
            "\n",
            "\n",
            "3. CASOS LÍMITE - MUY SUTILES\n",
            "----------------------------------------\n",
            "✓ 'No todos los estudiantes tienen la misma capacidad' → NORMAL (esperado: NORMAL)\n",
            "✓ 'Algunos alumnos necesitan más apoyo que otros' → NORMAL (esperado: NORMAL)\n",
            "✓ 'Tu rendimiento no es el esperado para alguien como tú' → OFENSIVO (esperado: OFENSIVO)\n",
            "✓ 'Las chicas suelen ser más ordenadas que los chicos' → OFENSIVO (esperado: OFENSIVO)\n",
            "✓ 'Los chicos son naturalmente más agresivos' → OFENSIVO (esperado: OFENSIVO)\n",
            "✓ 'Tu familia probablemente no valora la educación' → OFENSIVO (esperado: OFENSIVO)\n",
            "✗ 'Estudiantes de tu condición social suelen abandonar' → NORMAL (esperado: OFENSIVO)\n",
            "✓ 'No esperaba mucho de ti considerando tu origen' → OFENSIVO (esperado: OFENSIVO)\n",
            "\n",
            "\n",
            "4. CASOS CON NEGACIONES\n",
            "------------------------------\n",
            "✗ 'No vamos a discriminar a ningún estudiante' → OFENSIVO (esperado: NORMAL)\n",
            "✗ 'No toleraremos insultos en esta clase' → OFENSIVO (esperado: NORMAL)\n",
            "✗ 'No es cierto que las mujeres sean peores en matemáticas' → OFENSIVO (esperado: NORMAL)\n",
            "✓ 'No deberías comportarte como un mono' → OFENSIVO (esperado: OFENSIVO)\n",
            "✓ 'No me extraña que falles, eres de barrio' → OFENSIVO (esperado: OFENSIVO)\n",
            "✓ 'No esperaba nada bueno de tu tipo' → OFENSIVO (esperado: OFENSIVO)\n",
            "\n",
            "\n",
            "5. CONTEXTO ACADÉMICO ESPECÍFICO\n",
            "----------------------------------------\n",
            "✓ '('En anatomía estudiamos las diferencias entre sexos', 'NORMAL')' → NORMAL\n",
            "✓ '('Los caracteres sexuales secundarios aparecen en la pubertad', 'NORMAL')' → NORMAL\n",
            "✓ '('Las diferencias raciales son principalmente superficiales', 'NORMAL')' → NORMAL\n",
            "✗ '('La esclavitud fue una época oscura de la humanidad', 'NORMAL')' → OFENSIVO\n",
            "✓ '('Los conflictos étnicos han marcado la historia', 'NORMAL')' → NORMAL\n",
            "✗ '('El machismo ha sido un problema histórico', 'NORMAL')' → OFENSIVO\n",
            "✓ '('En esta obra el autor usa términos despectivos de la época', 'NORMAL')' → NORMAL\n",
            "✓ '('El personaje expresa ideas racistas que debemos analizar', 'NORMAL')' → NORMAL\n",
            "✓ '('La novela critica los estereotipos de género', 'NORMAL')' → NORMAL\n",
            "\n",
            "\n",
            "6. RESUMEN DE PRUEBAS\n",
            "------------------------------\n",
            "Precisión general: 43/53 (81.1%)\n",
            "✅ BUENO: El modelo funciona bien, pero se puede mejorar\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "81.13207547169812"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probar_modelo_completo(model_path, tokenizer_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9isfag6WQrMR"
      },
      "source": [
        "antes 81.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZGid8-rSMnF"
      },
      "source": [
        "# **RECONOCIMIENTO DE VOZ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU1_-DpMSzVA",
        "outputId": "4ce9e1a5-c4f7-422d-ad43-06e96df417e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Collecting pyannote.audio\n",
            "  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (24.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.15.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.8.1)\n",
            "Collecting lightning>=2.0.1 (from pyannote.audio)\n",
            "  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.3.0)\n",
            "Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n",
            "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n",
            "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n",
            "Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n",
            "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (13.9.4)\n",
            "Collecting semver>=3.0.0 (from pyannote.audio)\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting tensorboardX>=2.6 (from pyannote.audio)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n",
            "  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n",
            "  Downloading torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (1.1.5)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n",
            "Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.5)\n",
            "Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.41)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (2025.6.15)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n",
            "Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n",
            "Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: docopt, julius\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=56a35c448c5b67c4c39c61374a65b43bc526b2090ff0ffff3bff16be2610fb2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=35cd25c1f13d0476c8191d4edfe99f3b80fe18f085b5cde322989ca7cdefafe8\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\n",
            "Successfully built docopt julius\n",
            "Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, alembic, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n",
            "Successfully installed alembic-1.16.2 asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.2 lightning-utilities-0.14.3 optuna-4.4.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.2 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.7.3\n"
          ]
        }
      ],
      "source": [
        "## 1. Instalación de dependencias\n",
        "!pip install speechbrain torch librosa pyannote.audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnEltBNWTF8d",
        "outputId": "e5f8ccea-c5cc-42f7-8508-6ff7db4b6fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: speechbrain in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (24.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.15.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->speechbrain) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (1.1.5)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.11/dist-packages (from hyperpyyaml->speechbrain) (0.18.14)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->speechbrain) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install speechbrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnOg2fScSPfF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import json\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "from speechbrain.pretrained import SpeakerRecognition\n",
        "from collections import defaultdict\n",
        "import threading\n",
        "import time\n",
        "\n",
        "class StudentParticipationSystem:\n",
        "    def __init__(self, db_path=\"participation.db\", threshold=0.7):\n",
        "        \"\"\"\n",
        "        Sistema de control de participación estudiantil\n",
        "\n",
        "        Args:\n",
        "            db_path: Ruta de la base de datos\n",
        "            threshold: Umbral de similitud para considerar coincidencia (0.7 recomendado)\n",
        "        \"\"\"\n",
        "        self.threshold = threshold\n",
        "        self.db_path = db_path\n",
        "\n",
        "        # Determinar dispositivo\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Usando dispositivo: {self.device}\")\n",
        "\n",
        "        # Cargar modelo SpeechBrain\n",
        "        self.verifier = SpeakerRecognition.from_hparams(\n",
        "            source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "            run_opts={\"device\": str(self.device)}\n",
        "        )\n",
        "\n",
        "        # Diccionario para almacenar embeddings de estudiantes\n",
        "        self.student_embeddings = {}\n",
        "        self.student_names = {}\n",
        "\n",
        "        # Control de tiempo para evitar múltiples registros del mismo estudiante\n",
        "        self.last_detection = defaultdict(float)\n",
        "        self.cooldown_time = 30  # 30 segundos entre detecciones del mismo estudiante\n",
        "\n",
        "        # Inicializar base de datos\n",
        "        self.init_database()\n",
        "\n",
        "    def init_database(self):\n",
        "        \"\"\"Inicializa la base de datos SQLite\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Tabla de estudiantes\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS students (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                name TEXT UNIQUE NOT NULL,\n",
        "                embedding_path TEXT,\n",
        "                total_participations INTEGER DEFAULT 0\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Tabla de participaciones por clase\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS participations (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                student_id INTEGER,\n",
        "                class_date DATE,\n",
        "                timestamp DATETIME,\n",
        "                confidence_score REAL,\n",
        "                FOREIGN KEY (student_id) REFERENCES students (id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Tabla de clases\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS classes (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                class_name TEXT,\n",
        "                date DATE,\n",
        "                start_time TIME,\n",
        "                end_time TIME\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def register_student(self, student_id, student_name, audio_samples):\n",
        "        \"\"\"\n",
        "        Registra un nuevo estudiante con múltiples muestras de audio\n",
        "\n",
        "        Args:\n",
        "            student_id: ID único del estudiante\n",
        "            student_name: Nombre del estudiante\n",
        "            audio_samples: Lista de rutas de archivos de audio del estudiante\n",
        "        \"\"\"\n",
        "        print(f\"Registrando estudiante: {student_name}\")\n",
        "\n",
        "        # Procesar múltiples muestras para crear un embedding promedio\n",
        "        embeddings = []\n",
        "\n",
        "        for audio_path in audio_samples:\n",
        "            try:\n",
        "                # Cargar audio\n",
        "                audio, sr = librosa.load(audio_path, sr=16000)\n",
        "                audio_tensor = torch.from_numpy(audio).unsqueeze(0).to(self.device)\n",
        "\n",
        "                # Extraer embedding\n",
        "                embedding = self.verifier.encode_batch(audio_tensor)\n",
        "                embeddings.append(embedding.squeeze().cpu().numpy())\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error procesando {audio_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if embeddings:\n",
        "            # Promedio de embeddings para mayor robustez\n",
        "            avg_embedding = np.mean(embeddings, axis=0)\n",
        "            self.student_embeddings[student_id] = torch.from_numpy(avg_embedding).unsqueeze(0).to(self.device)\n",
        "            self.student_names[student_id] = student_name\n",
        "\n",
        "            # Guardar en base de datos\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            cursor.execute('''\n",
        "                INSERT OR REPLACE INTO students (id, name, total_participations)\n",
        "                VALUES (?, ?, 0)\n",
        "            ''', (student_id, student_name))\n",
        "\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "\n",
        "            print(f\"Estudiante {student_name} registrado exitosamente\")\n",
        "        else:\n",
        "            print(f\"Error: No se pudieron procesar las muestras de audio para {student_name}\")\n",
        "\n",
        "    def load_students_from_directory(self, base_directory):\n",
        "        \"\"\"\n",
        "        Carga estudiantes desde un directorio organizado\n",
        "        Estructura esperada: base_directory/student_id/audio1.wav, audio2.wav, ...\n",
        "        \"\"\"\n",
        "        import os\n",
        "\n",
        "        for student_folder in os.listdir(base_directory):\n",
        "            student_path = os.path.join(base_directory, student_folder)\n",
        "\n",
        "            if os.path.isdir(student_path):\n",
        "                try:\n",
        "                    student_id = int(student_folder)\n",
        "                    audio_files = [\n",
        "                        os.path.join(student_path, f)\n",
        "                        for f in os.listdir(student_path)\n",
        "                        if f.endswith(('.wav', '.mp3', '.flac'))\n",
        "                    ]\n",
        "\n",
        "                    if audio_files:\n",
        "                        # Usar el nombre de la carpeta como nombre del estudiante\n",
        "                        # En producción, podrías cargar esto desde un archivo CSV\n",
        "                        student_name = f\"Estudiante_{student_id}\"\n",
        "                        self.register_student(student_id, student_name, audio_files[:3])  # Máximo 3 muestras\n",
        "\n",
        "                except ValueError:\n",
        "                    print(f\"Ignorando carpeta con nombre inválido: {student_folder}\")\n",
        "\n",
        "    def identify_speaker(self, audio_path):\n",
        "        \"\"\"\n",
        "        Identifica al hablante en un archivo de audio\n",
        "\n",
        "        Args:\n",
        "            audio_path: Ruta del archivo de audio a analizar\n",
        "\n",
        "        Returns:\n",
        "            tuple: (student_id, confidence_score) o (None, 0) si no se encuentra coincidencia\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Cargar audio\n",
        "            audio, sr = librosa.load(audio_path, sr=16000)\n",
        "            audio_tensor = torch.from_numpy(audio).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # Extraer embedding del audio de entrada\n",
        "            input_embedding = self.verifier.encode_batch(audio_tensor)\n",
        "\n",
        "            best_match = None\n",
        "            best_score = 0\n",
        "\n",
        "            # Comparar con todos los estudiantes registrados\n",
        "            for student_id, student_embedding in self.student_embeddings.items():\n",
        "                # Asegurar que ambos embeddings estén en el mismo dispositivo\n",
        "                input_emb = input_embedding.squeeze().to(self.device)\n",
        "                student_emb = student_embedding.squeeze().to(self.device)\n",
        "\n",
        "                # Calcular similitud coseno\n",
        "                similarity = torch.cosine_similarity(\n",
        "                    input_emb,\n",
        "                    student_emb,\n",
        "                    dim=0\n",
        "                ).item()\n",
        "\n",
        "                if similarity > best_score and similarity > self.threshold:\n",
        "                    best_score = similarity\n",
        "                    best_match = student_id\n",
        "\n",
        "            return best_match, best_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error identificando hablante: {e}\")\n",
        "            return None, 0\n",
        "\n",
        "    def record_participation(self, student_id, confidence_score, class_date=None):\n",
        "        \"\"\"\n",
        "        Registra una participación en la base de datos\n",
        "\n",
        "        Args:\n",
        "            student_id: ID del estudiante\n",
        "            confidence_score: Puntuación de confianza\n",
        "            class_date: Fecha de la clase (por defecto hoy)\n",
        "        \"\"\"\n",
        "        if class_date is None:\n",
        "            class_date = datetime.now().date()\n",
        "\n",
        "        current_time = time.time()\n",
        "\n",
        "        # Verificar cooldown para evitar registros duplicados\n",
        "        if current_time - self.last_detection[student_id] < self.cooldown_time:\n",
        "            return False\n",
        "\n",
        "        self.last_detection[student_id] = current_time\n",
        "\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Registrar participación\n",
        "        cursor.execute('''\n",
        "            INSERT INTO participations (student_id, class_date, timestamp, confidence_score)\n",
        "            VALUES (?, ?, ?, ?)\n",
        "        ''', (student_id, class_date, datetime.now(), confidence_score))\n",
        "\n",
        "        # Actualizar contador total\n",
        "        cursor.execute('''\n",
        "            UPDATE students\n",
        "            SET total_participations = total_participations + 1\n",
        "            WHERE id = ?\n",
        "        ''', (student_id,))\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "        student_name = self.student_names.get(student_id, f\"ID_{student_id}\")\n",
        "        print(f\"✓ Participación registrada: {student_name} (Confianza: {confidence_score:.3f})\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def process_audio_stream(self, audio_path):\n",
        "        \"\"\"\n",
        "        Procesa un archivo de audio y registra participaciones\n",
        "\n",
        "        Args:\n",
        "            audio_path: Ruta del archivo de audio a procesar\n",
        "        \"\"\"\n",
        "        print(f\"Procesando audio: {audio_path}\")\n",
        "\n",
        "        student_id, confidence = self.identify_speaker(audio_path)\n",
        "\n",
        "        if student_id:\n",
        "            self.record_participation(student_id, confidence)\n",
        "        else:\n",
        "            print(\"No se identificó ningún estudiante registrado\")\n",
        "\n",
        "    def get_participation_report(self, class_date=None):\n",
        "        \"\"\"\n",
        "        Genera un reporte de participación\n",
        "\n",
        "        Args:\n",
        "            class_date: Fecha específica (por defecto hoy)\n",
        "\n",
        "        Returns:\n",
        "            list: Lista de tuplas (nombre, participaciones)\n",
        "        \"\"\"\n",
        "        if class_date is None:\n",
        "            class_date = datetime.now().date()\n",
        "\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            SELECT s.name, COUNT(p.id) as participations\n",
        "            FROM students s\n",
        "            LEFT JOIN participations p ON s.id = p.student_id\n",
        "                AND p.class_date = ?\n",
        "            GROUP BY s.id, s.name\n",
        "            ORDER BY participations DESC\n",
        "        ''', (class_date,))\n",
        "\n",
        "        results = cursor.fetchall()\n",
        "        conn.close()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_participation_stats(self, class_date=None):\n",
        "        \"\"\"Imprime estadísticas de participación\"\"\"\n",
        "        if class_date is None:\n",
        "            class_date = datetime.now().date()\n",
        "\n",
        "        print(f\"\\n=== REPORTE DE PARTICIPACIÓN - {class_date} ===\")\n",
        "\n",
        "        report = self.get_participation_report(class_date)\n",
        "\n",
        "        for name, participations in report[:10]:  # Top 10\n",
        "            print(f\"{name}: {participations} participaciones\")\n",
        "\n",
        "        total_students = len(self.student_embeddings)\n",
        "        active_students = sum(1 for _, p in report if p > 0)\n",
        "\n",
        "        print(f\"\\nResumen:\")\n",
        "        print(f\"- Estudiantes registrados: {total_students}\")\n",
        "        print(f\"- Estudiantes que participaron: {active_students}\")\n",
        "        print(f\"- Tasa de participación: {active_students/total_students*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8LFAj7Xw-t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import json\n",
        "import sqlite3\n",
        "import pyaudio\n",
        "from datetime import datetime\n",
        "from resemblyzer import VoiceEncoder, preprocess_wav\n",
        "from collections import defaultdict\n",
        "import threading\n",
        "import time\n",
        "import wave\n",
        "import tempfile\n",
        "import os\n",
        "from queue import Queue\n",
        "\n",
        "class StudentParticipationSystem:\n",
        "    def __init__(self, db_path=\"participation.db\", threshold=0.7):\n",
        "        \"\"\"\n",
        "        Sistema de control de participación estudiantil\n",
        "\n",
        "        Args:\n",
        "            db_path: Ruta de la base de datos\n",
        "            threshold: Umbral de similitud para considerar coincidencia (0.7 recomendado)\n",
        "        \"\"\"\n",
        "        self.threshold = threshold\n",
        "        self.db_path = db_path\n",
        "\n",
        "        # Determinar dispositivo\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Usando dispositivo: {self.device}\")\n",
        "\n",
        "        # Cargar modelo Resemblyzer\n",
        "        self.encoder = VoiceEncoder()\n",
        "\n",
        "        # Diccionario para almacenar embeddings de estudiantes\n",
        "        self.student_embeddings = {}\n",
        "        self.student_names = {}\n",
        "\n",
        "        # Control de tiempo para evitar múltiples registros del mismo estudiante\n",
        "        self.last_detection = defaultdict(float)\n",
        "        self.cooldown_time = 30  # 30 segundos entre detecciones del mismo estudiante\n",
        "\n",
        "        # Variables para captura de audio en tiempo real\n",
        "        self.is_recording = False\n",
        "        self.audio_queue = Queue()\n",
        "        self.sample_rate = 16000\n",
        "        self.chunk_size = 1024\n",
        "        self.channels = 1\n",
        "\n",
        "        # Inicializar base de datos\n",
        "        self.init_database()\n",
        "\n",
        "    def init_database(self):\n",
        "        \"\"\"Inicializa la base de datos SQLite\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Tabla de estudiantes\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS students (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                name TEXT UNIQUE NOT NULL,\n",
        "                embedding_path TEXT,\n",
        "                total_participations INTEGER DEFAULT 0\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Tabla de participaciones por clase\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS participations (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                student_id INTEGER,\n",
        "                class_date DATE,\n",
        "                timestamp DATETIME,\n",
        "                confidence_score REAL,\n",
        "                FOREIGN KEY (student_id) REFERENCES students (id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Tabla de clases\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS classes (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                class_name TEXT,\n",
        "                date DATE,\n",
        "                start_time TIME,\n",
        "                end_time TIME\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def register_student(self, student_id, student_name, audio_samples):\n",
        "        \"\"\"\n",
        "        Registra un nuevo estudiante con múltiples muestras de audio\n",
        "\n",
        "        Args:\n",
        "            student_id: ID único del estudiante\n",
        "            student_name: Nombre del estudiante\n",
        "            audio_samples: Lista de rutas de archivos de audio del estudiante\n",
        "        \"\"\"\n",
        "        print(f\"Registrando estudiante: {student_name}\")\n",
        "\n",
        "        # Procesar múltiples muestras para crear un embedding promedio\n",
        "        embeddings = []\n",
        "\n",
        "        for audio_path in audio_samples:\n",
        "            try:\n",
        "                # Cargar y preprocesar audio con Resemblyzer\n",
        "                wav = preprocess_wav(audio_path)\n",
        "\n",
        "                # Extraer embedding\n",
        "                embedding = self.encoder.embed_utterance(wav)\n",
        "                embeddings.append(embedding)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error procesando {audio_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if embeddings:\n",
        "            # Promedio de embeddings para mayor robustez\n",
        "            avg_embedding = np.mean(embeddings, axis=0)\n",
        "            self.student_embeddings[student_id] = avg_embedding\n",
        "            self.student_names[student_id] = student_name\n",
        "\n",
        "            # Guardar en base de datos\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            cursor.execute('''\n",
        "                INSERT OR REPLACE INTO students (id, name, total_participations)\n",
        "                VALUES (?, ?, 0)\n",
        "            ''', (student_id, student_name))\n",
        "\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "\n",
        "            print(f\"Estudiante {student_name} registrado exitosamente\")\n",
        "        else:\n",
        "            print(f\"Error: No se pudieron procesar las muestras de audio para {student_name}\")\n",
        "\n",
        "    def load_students_from_directory(self, base_directory):\n",
        "        \"\"\"\n",
        "        Carga estudiantes desde un directorio organizado\n",
        "        Estructura esperada: base_directory/student_id/audio1.wav, audio2.wav, ...\n",
        "        \"\"\"\n",
        "        import os\n",
        "\n",
        "        for student_folder in os.listdir(base_directory):\n",
        "            student_path = os.path.join(base_directory, student_folder)\n",
        "\n",
        "            if os.path.isdir(student_path):\n",
        "                try:\n",
        "                    student_id = int(student_folder)\n",
        "                    audio_files = [\n",
        "                        os.path.join(student_path, f)\n",
        "                        for f in os.listdir(student_path)\n",
        "                        if f.endswith(('.wav', '.mp3', '.flac'))\n",
        "                    ]\n",
        "\n",
        "                    if audio_files:\n",
        "                        # Usar el nombre de la carpeta como nombre del estudiante\n",
        "                        # En producción, podrías cargar esto desde un archivo CSV\n",
        "                        student_name = f\"Estudiante_{student_id}\"\n",
        "                        self.register_student(student_id, student_name, audio_files[:3])  # Máximo 3 muestras\n",
        "\n",
        "                except ValueError:\n",
        "                    print(f\"Ignorando carpeta con nombre inválido: {student_folder}\")\n",
        "\n",
        "    def identify_speaker(self, audio_path):\n",
        "        \"\"\"\n",
        "        Identifica al hablante en un archivo de audio\n",
        "\n",
        "        Args:\n",
        "            audio_path: Ruta del archivo de audio a analizar\n",
        "\n",
        "        Returns:\n",
        "            tuple: (student_id, confidence_score) o (None, 0) si no se encuentra coincidencia\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Cargar y preprocesar audio con Resemblyzer\n",
        "            wav = preprocess_wav(audio_path)\n",
        "\n",
        "            # Extraer embedding del audio de entrada\n",
        "            input_embedding = self.encoder.embed_utterance(wav)\n",
        "\n",
        "            best_match = None\n",
        "            best_score = 0\n",
        "\n",
        "            # Comparar con todos los estudiantes registrados\n",
        "            for student_id, student_embedding in self.student_embeddings.items():\n",
        "                # Calcular similitud coseno\n",
        "                similarity = np.dot(input_embedding, student_embedding) / (\n",
        "                    np.linalg.norm(input_embedding) * np.linalg.norm(student_embedding)\n",
        "                )\n",
        "\n",
        "                if similarity > best_score and similarity > self.threshold:\n",
        "                    best_score = similarity\n",
        "                    best_match = student_id\n",
        "\n",
        "            return best_match, best_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error identificando hablante: {e}\")\n",
        "            return None, 0\n",
        "\n",
        "    def record_participation(self, student_id, confidence_score, class_date=None):\n",
        "        \"\"\"\n",
        "        Registra una participación en la base de datos\n",
        "\n",
        "        Args:\n",
        "            student_id: ID del estudiante\n",
        "            confidence_score: Puntuación de confianza\n",
        "            class_date: Fecha de la clase (por defecto hoy)\n",
        "        \"\"\"\n",
        "        if class_date is None:\n",
        "            class_date = datetime.now().date()\n",
        "\n",
        "        current_time = time.time()\n",
        "\n",
        "        # Verificar cooldown para evitar registros duplicados\n",
        "        if current_time - self.last_detection[student_id] < self.cooldown_time:\n",
        "            return False\n",
        "\n",
        "        self.last_detection[student_id] = current_time\n",
        "\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Registrar participación\n",
        "        cursor.execute('''\n",
        "            INSERT INTO participations (student_id, class_date, timestamp, confidence_score)\n",
        "            VALUES (?, ?, ?, ?)\n",
        "        ''', (student_id, class_date, datetime.now(), confidence_score))\n",
        "\n",
        "        # Actualizar contador total\n",
        "        cursor.execute('''\n",
        "            UPDATE students\n",
        "            SET total_participations = total_participations + 1\n",
        "            WHERE id = ?\n",
        "        ''', (student_id,))\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "        student_name = self.student_names.get(student_id, f\"ID_{student_id}\")\n",
        "        print(f\"✓ Participación registrada: {student_name} (Confianza: {confidence_score:.3f})\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def detect_voice_activity(self, audio_data, threshold=0.01):\n",
        "        \"\"\"\n",
        "        Detecta si hay actividad de voz en el audio\n",
        "\n",
        "        Args:\n",
        "            audio_data: Array de numpy con datos de audio\n",
        "            threshold: Umbral de energía para detectar voz\n",
        "\n",
        "        Returns:\n",
        "            bool: True si se detecta voz\n",
        "        \"\"\"\n",
        "        # Calcular energía RMS\n",
        "        rms = np.sqrt(np.mean(audio_data**2))\n",
        "        return rms > threshold\n",
        "\n",
        "    def record_audio_chunk(self, duration=3):\n",
        "        \"\"\"\n",
        "        Graba un chunk de audio del micrófono usando Google Colab\n",
        "\n",
        "        Args:\n",
        "            duration: Duración en segundos del chunk\n",
        "\n",
        "        Returns:\n",
        "            str: Ruta del archivo temporal con el audio grabado\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from google.colab import output\n",
        "            from IPython.display import Javascript\n",
        "            import base64\n",
        "            import io\n",
        "\n",
        "            # JavaScript para grabar audio en Colab\n",
        "            RECORD = \"\"\"\n",
        "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "            const b2text = blob => new Promise(resolve => {\n",
        "              const reader = new FileReader()\n",
        "              reader.onloadend = e => resolve(e.srcElement.result)\n",
        "              reader.readAsDataURL(blob)\n",
        "            })\n",
        "\n",
        "            var record = time => new Promise(async resolve => {\n",
        "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "              recorder = new MediaRecorder(stream)\n",
        "              chunks = []\n",
        "              recorder.ondataavailable = e => chunks.push(e.data)\n",
        "              recorder.start()\n",
        "              await sleep(time)\n",
        "              recorder.onstop = async ()=>{\n",
        "                blob = new Blob(chunks)\n",
        "                text = await b2text(blob)\n",
        "                resolve(text)\n",
        "              }\n",
        "              recorder.stop()\n",
        "            })\n",
        "            \"\"\"\n",
        "\n",
        "            print(f\"🎤 Grabando audio por {duration} segundos...\")\n",
        "            display(Javascript(RECORD))\n",
        "            s = output.eval_js('record(%d)' % (duration * 1000))\n",
        "\n",
        "            if not s:\n",
        "                return None\n",
        "\n",
        "            # Decodificar base64\n",
        "            b = base64.b64decode(s.split(',')[1])\n",
        "\n",
        "            # Guardar en archivo temporal\n",
        "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')\n",
        "            temp_path = temp_file.name\n",
        "            temp_file.write(b)\n",
        "            temp_file.close()\n",
        "\n",
        "            # Cargar con librosa para verificar actividad de voz\n",
        "            try:\n",
        "                audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
        "\n",
        "                # Verificar si hay actividad de voz\n",
        "                if not self.detect_voice_activity(audio_data):\n",
        "                    os.unlink(temp_path)\n",
        "                    return None\n",
        "\n",
        "                return temp_path\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error procesando audio: {e}\")\n",
        "                os.unlink(temp_path)\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error grabando audio: {e}\")\n",
        "            return None\n",
        "\n",
        "    def start_real_time_monitoring(self, chunk_duration=3, check_interval=1):\n",
        "        \"\"\"\n",
        "        Inicia el monitoreo en tiempo real del micrófono\n",
        "\n",
        "        Args:\n",
        "            chunk_duration: Duración de cada chunk de audio a analizar\n",
        "            check_interval: Intervalo entre grabaciones en segundos\n",
        "        \"\"\"\n",
        "        print(\"🚀 Iniciando monitoreo en tiempo real...\")\n",
        "        print(\"Presiona Ctrl+C para detener\")\n",
        "\n",
        "        self.is_recording = True\n",
        "\n",
        "        try:\n",
        "            while self.is_recording:\n",
        "                # Grabar chunk de audio\n",
        "                audio_path = self.record_audio_chunk(chunk_duration)\n",
        "\n",
        "                if audio_path:\n",
        "                    # Procesar audio para identificar estudiante\n",
        "                    student_id, confidence = self.identify_speaker(audio_path)\n",
        "\n",
        "                    if student_id:\n",
        "                        student_name = self.student_names.get(student_id, f\"ID_{student_id}\")\n",
        "                        print(f\"🗣️  Detectado: {student_name} (Confianza: {confidence:.3f})\")\n",
        "\n",
        "                        # Registrar participación\n",
        "                        if self.record_participation(student_id, confidence):\n",
        "                            print(f\"✅ Participación registrada para {student_name}\")\n",
        "                    else:\n",
        "                        print(\"👤 Voz detectada pero no identificada\")\n",
        "\n",
        "                    # Limpiar archivo temporal\n",
        "                    os.unlink(audio_path)\n",
        "                else:\n",
        "                    print(\"🔇 Sin actividad de voz detectada\")\n",
        "\n",
        "                # Esperar antes del siguiente chunk\n",
        "                time.sleep(check_interval)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n⏹️  Deteniendo monitoreo...\")\n",
        "            self.is_recording = False\n",
        "        except Exception as e:\n",
        "            print(f\"Error en monitoreo: {e}\")\n",
        "            self.is_recording = False\n",
        "\n",
        "    def stop_monitoring(self):\n",
        "        \"\"\"Detiene el monitoreo en tiempo real\"\"\"\n",
        "        self.is_recording = False\n",
        "        print(\"Monitoreo detenido\")\n",
        "\n",
        "    def start_monitoring_thread(self, chunk_duration=3, check_interval=1):\n",
        "        \"\"\"\n",
        "        Inicia el monitoreo en un hilo separado\n",
        "\n",
        "        Args:\n",
        "            chunk_duration: Duración de cada chunk de audio\n",
        "            check_interval: Intervalo entre grabaciones\n",
        "        \"\"\"\n",
        "        monitoring_thread = threading.Thread(\n",
        "            target=self.start_real_time_monitoring,\n",
        "            args=(chunk_duration, check_interval)\n",
        "        )\n",
        "        monitoring_thread.daemon = True\n",
        "        monitoring_thread.start()\n",
        "        return monitoring_thread\n",
        "\n",
        "    def get_participation_report(self, class_date=None):\n",
        "        \"\"\"\n",
        "        Genera un reporte de participación\n",
        "\n",
        "        Args:\n",
        "            class_date: Fecha específica (por defecto hoy)\n",
        "\n",
        "        Returns:\n",
        "            list: Lista de tuplas (nombre, participaciones)\n",
        "        \"\"\"\n",
        "        if class_date is None:\n",
        "            class_date = datetime.now().date()\n",
        "\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            SELECT s.name, COUNT(p.id) as participations\n",
        "            FROM students s\n",
        "            LEFT JOIN participations p ON s.id = p.student_id\n",
        "                AND p.class_date = ?\n",
        "            GROUP BY s.id, s.name\n",
        "            ORDER BY participations DESC\n",
        "        ''', (class_date,))\n",
        "\n",
        "        results = cursor.fetchall()\n",
        "        conn.close()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_participation_stats(self, class_date=None):\n",
        "        \"\"\"Imprime estadísticas de participación\"\"\"\n",
        "        if class_date is None:\n",
        "            class_date = datetime.now().date()\n",
        "\n",
        "        print(f\"\\n=== REPORTE DE PARTICIPACIÓN - {class_date} ===\")\n",
        "\n",
        "        report = self.get_participation_report(class_date)\n",
        "\n",
        "        for name, participations in report[:10]:  # Top 10\n",
        "            print(f\"{name}: {participations} participations\")\n",
        "\n",
        "        total_students = len(self.student_embeddings)\n",
        "        active_students = sum(1 for _, p in report if p > 0)\n",
        "\n",
        "        print(f\"\\nResumen:\")\n",
        "        print(f\"- Estudiantes registrados: {total_students}\")\n",
        "        print(f\"- Estudiantes que participaron: {active_students}\")\n",
        "        print(f\"- Tasa de participación: {active_students/total_students*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--Z_VcM7YThC",
        "outputId": "0b7187f5-2237-4d3e-bf12-c5ada209c19e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando dispositivo: cuda\n",
            "Loaded the voice encoder model on cuda in 0.02 seconds.\n"
          ]
        }
      ],
      "source": [
        "# Inicializar sistema\n",
        "system = StudentParticipationSystem(threshold=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkLK3OwPYghw",
        "outputId": "34160b35-5c40-4711-c24b-b71fa372b70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 1s (197 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaudio: filename=pyaudio-0.2.14-cp311-cp311-linux_x86_64.whl size=67423 sha256=13dadf5958f7f617788a33fa11e6901e808c709dd586f994cef05f6e5d678594\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/b1/c1/67e4ef443de2665d86031d4760508094eab5de37d5d64d9c27\n",
            "Successfully built pyaudio\n",
            "Installing collected packages: pyaudio\n",
            "Successfully installed pyaudio-0.2.14\n"
          ]
        }
      ],
      "source": [
        "# En Google Colab\n",
        "!apt-get install -y portaudio19-dev\n",
        "!pip install pyaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0g6sl8lX_o1",
        "outputId": "8bd6a761-ab73-4b44-ed0d-268307bb0dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registrando estudiante: Manuel Rua\n",
            "Estudiante Manuel Rua registrado exitosamente\n",
            "Registrando estudiante: Atzel Cervantes\n",
            "Estudiante Atzel Cervantes registrado exitosamente\n",
            "Registrando estudiante: Elmer Vela\n",
            "Estudiante Elmer Vela registrado exitosamente\n",
            "Registrando estudiante: Jorge Mendez\n",
            "Estudiante Jorge Mendez registrado exitosamente\n",
            "Registrando estudiante: Estudiante_1\n",
            "Estudiante Estudiante_1 registrado exitosamente\n",
            "Registrando estudiante: Estudiante_3\n",
            "Estudiante Estudiante_3 registrado exitosamente\n",
            "Registrando estudiante: Estudiante_2\n",
            "Estudiante Estudiante_2 registrado exitosamente\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de registro manual de estudiantes\n",
        "system.register_student(\n",
        "     student_id=5,\n",
        "     student_name=\"Manuel Rua\",\n",
        "     audio_samples=[\"/content/drive/MyDrive/Manuel.wav\"]\n",
        ")\n",
        "\n",
        "system.register_student(\n",
        "     student_id=6,\n",
        "     student_name=\"Atzel Cervantes\",\n",
        "     audio_samples=[\"/content/drive/MyDrive/Atzel.wav\"]\n",
        ")\n",
        "\n",
        "system.register_student(\n",
        "     student_id=7,\n",
        "     student_name=\"Elmer Vela\",\n",
        "     audio_samples=[\"/content/drive/MyDrive/Pirlo.wav\"]\n",
        ")\n",
        "\n",
        "system.register_student(\n",
        "     student_id=8,\n",
        "     student_name=\"Jorge Mendez\",\n",
        "     audio_samples=[\"/content/drive/MyDrive/Mendes.wav\"]\n",
        ")\n",
        "\n",
        "    # Cargar estudiantes desde directorio (método recomendado)\n",
        "system.load_students_from_directory(\"/content/drive/MyDrive/students_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W5CzkacAYuDt",
        "outputId": "758282e9-8859-4605-f29e-e5a422d579ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Iniciando monitoreo en tiempo real...\n",
            "Presiona Ctrl+C para detener\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🗣️  Detectado: Manuel Rua (Confianza: 0.715)\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🗣️  Detectado: Jorge Mendez (Confianza: 0.735)\n",
            "✓ Participación registrada: Jorge Mendez (Confianza: 0.735)\n",
            "✅ Participación registrada para Jorge Mendez\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👤 Voz detectada pero no identificada\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👤 Voz detectada pero no identificada\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🗣️  Detectado: Jorge Mendez (Confianza: 0.775)\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👤 Voz detectada pero no identificada\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👤 Voz detectada pero no identificada\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👤 Voz detectada pero no identificada\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🗣️  Detectado: Elmer Vela (Confianza: 0.701)\n",
            "✓ Participación registrada: Elmer Vela (Confianza: 0.701)\n",
            "✅ Participación registrada para Elmer Vela\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👤 Voz detectada pero no identificada\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👤 Voz detectada pero no identificada\n",
            "🎤 Grabando audio por 6 segundos...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "            const b2text = blob => new Promise(resolve => {\n",
              "              const reader = new FileReader()\n",
              "              reader.onloadend = e => resolve(e.srcElement.result)\n",
              "              reader.readAsDataURL(blob)\n",
              "            })\n",
              "\n",
              "            var record = time => new Promise(async resolve => {\n",
              "              stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "              recorder = new MediaRecorder(stream)\n",
              "              chunks = []\n",
              "              recorder.ondataavailable = e => chunks.push(e.data)\n",
              "              recorder.start()\n",
              "              await sleep(time)\n",
              "              recorder.onstop = async ()=>{\n",
              "                blob = new Blob(chunks)\n",
              "                text = await b2text(blob)\n",
              "                resolve(text)\n",
              "              }\n",
              "              recorder.stop()\n",
              "            })\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3341603086.py:327: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.11/dist-packages/resemblyzer/audio.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👤 Voz detectada pero no identificada\n",
            "\n",
            "⏹️  Deteniendo monitoreo...\n"
          ]
        }
      ],
      "source": [
        "# INICIAR MONITOREO EN TIEMPO REAL\n",
        "system.start_real_time_monitoring(\n",
        "    chunk_duration=6,     # Analiza cada 3 segundos\n",
        "    check_interval=2    # Espera 0.5s entre análisis\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88H4nzyncMRb"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "from IPython.display import Javascript, display\n",
        "import base64\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z71TA6I9kOUo",
        "outputId": "429bf219-8af9-4fa4-e056-7248fe707dfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting resemblyzer\n",
            "  Downloading Resemblyzer-0.1.4-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: librosa>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from resemblyzer) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from resemblyzer) (2.0.2)\n",
            "Collecting webrtcvad>=2.0.10 (from resemblyzer)\n",
            "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from resemblyzer) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from resemblyzer) (1.15.3)\n",
            "Collecting typing (from resemblyzer)\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (4.14.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer) (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->resemblyzer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.1->resemblyzer) (1.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa>=0.9.1->resemblyzer) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa>=0.9.1->resemblyzer) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.9.1->resemblyzer) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.9.1->resemblyzer) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa>=0.9.1->resemblyzer) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa>=0.9.1->resemblyzer) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.1->resemblyzer) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.9.1->resemblyzer) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->resemblyzer) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->resemblyzer) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->resemblyzer) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->resemblyzer) (2025.6.15)\n",
            "Downloading Resemblyzer-0.1.4-py3-none-any.whl (15.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: webrtcvad, typing\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp311-cp311-linux_x86_64.whl size=73508 sha256=b01fdc781bf1691fbca250d3be5ced2f595e11080ed246583e3f81d51f6a26b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/65/3f/292d0b656be33d1c801831201c74b5f68f41a2ae465ff2ee2f\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=5c9983605500517fa0dcd8d1697e52c9554486defcbc873a9cccbe811b7d0d58\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/67/2f/53e3ef32ec48d11d7d60245255e2d71e908201d20c880c08ee\n",
            "Successfully built webrtcvad typing\n",
            "Installing collected packages: webrtcvad, typing, resemblyzer\n",
            "Successfully installed resemblyzer-0.1.4 typing-3.7.4.3 webrtcvad-2.0.10\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "21921490e4c344d5ac11fff8bf660e16",
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install resemblyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDioEmExkGqa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import json\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "from resemblyzer import VoiceEncoder, preprocess_wav\n",
        "from collections import defaultdict\n",
        "import threading\n",
        "import time\n",
        "\n",
        "class StudentParticipationSystem:\n",
        "    def __init__(self, db_path=\"participation.db\", threshold=0.7):\n",
        "        \"\"\"\n",
        "        Sistema de control de participación estudiantil\n",
        "\n",
        "        Args:\n",
        "            db_path: Ruta de la base de datos\n",
        "            threshold: Umbral de similitud para considerar coincidencia (0.7 recomendado)\n",
        "        \"\"\"\n",
        "        self.threshold = threshold\n",
        "        self.db_path = db_path\n",
        "\n",
        "        # Determinar dispositivo\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Usando dispositivo: {self.device}\")\n",
        "\n",
        "        # Cargar modelo Resemblyzer\n",
        "        self.encoder = VoiceEncoder()\n",
        "\n",
        "        # Diccionario para almacenar embeddings de estudiantes\n",
        "        self.student_embeddings = {}\n",
        "        self.student_names = {}\n",
        "\n",
        "        # Control de tiempo para evitar múltiples registros del mismo estudiante\n",
        "        self.last_detection = defaultdict(float)\n",
        "        self.cooldown_time = 30  # 30 segundos entre detecciones del mismo estudiante\n",
        "\n",
        "        # Inicializar base de datos\n",
        "        self.init_database()\n",
        "\n",
        "    def init_database(self):\n",
        "        \"\"\"Inicializa la base de datos SQLite\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Tabla de estudiantes\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS students (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                name TEXT UNIQUE NOT NULL,\n",
        "                embedding_path TEXT,\n",
        "                total_participations INTEGER DEFAULT 0\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Tabla de participaciones por clase\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS participations (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                student_id INTEGER,\n",
        "                class_date DATE,\n",
        "                timestamp DATETIME,\n",
        "                confidence_score REAL,\n",
        "                FOREIGN KEY (student_id) REFERENCES students (id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Tabla de clases\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS classes (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                class_name TEXT,\n",
        "                date DATE,\n",
        "                start_time TIME,\n",
        "                end_time TIME\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def register_student(self, student_id, student_name, audio_samples):\n",
        "        \"\"\"\n",
        "        Registra un nuevo estudiante con múltiples muestras de audio\n",
        "\n",
        "        Args:\n",
        "            student_id: ID único del estudiante\n",
        "            student_name: Nombre del estudiante\n",
        "            audio_samples: Lista de rutas de archivos de audio del estudiante\n",
        "        \"\"\"\n",
        "        print(f\"Registrando estudiante: {student_name}\")\n",
        "\n",
        "        # Procesar múltiples muestras para crear un embedding promedio\n",
        "        embeddings = []\n",
        "\n",
        "        for audio_path in audio_samples:\n",
        "            try:\n",
        "                # Cargar y preprocesar audio con Resemblyzer\n",
        "                wav = preprocess_wav(audio_path)\n",
        "\n",
        "                # Extraer embedding\n",
        "                embedding = self.encoder.embed_utterance(wav)\n",
        "                embeddings.append(embedding)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error procesando {audio_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if embeddings:\n",
        "            # Promedio de embeddings para mayor robustez\n",
        "            avg_embedding = np.mean(embeddings, axis=0)\n",
        "            self.student_embeddings[student_id] = avg_embedding\n",
        "            self.student_names[student_id] = student_name\n",
        "\n",
        "            # Guardar en base de datos\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            cursor.execute('''\n",
        "                INSERT OR REPLACE INTO students (id, name, total_participations)\n",
        "                VALUES (?, ?, 0)\n",
        "            ''', (student_id, student_name))\n",
        "\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "\n",
        "            print(f\"Estudiante {student_name} registrado exitosamente\")\n",
        "        else:\n",
        "            print(f\"Error: No se pudieron procesar las muestras de audio para {student_name}\")\n",
        "\n",
        "    def load_students_from_directory(self, base_directory):\n",
        "        \"\"\"\n",
        "        Carga estudiantes desde un directorio organizado\n",
        "        Estructura esperada: base_directory/student_id/audio1.wav, audio2.wav, ...\n",
        "        \"\"\"\n",
        "        import os\n",
        "\n",
        "        for student_folder in os.listdir(base_directory):\n",
        "            student_path = os.path.join(base_directory, student_folder)\n",
        "\n",
        "            if os.path.isdir(student_path):\n",
        "                try:\n",
        "                    student_id = int(student_folder)\n",
        "                    audio_files = [\n",
        "                        os.path.join(student_path, f)\n",
        "                        for f in os.listdir(student_path)\n",
        "                        if f.endswith(('.wav', '.mp3', '.flac'))\n",
        "                    ]\n",
        "\n",
        "                    if audio_files:\n",
        "                        # Usar el nombre de la carpeta como nombre del estudiante\n",
        "                        # En producción, podrías cargar esto desde un archivo CSV\n",
        "                        student_name = f\"Estudiante_{student_id}\"\n",
        "                        self.register_student(student_id, student_name, audio_files[:3])  # Máximo 3 muestras\n",
        "\n",
        "                except ValueError:\n",
        "                    print(f\"Ignorando carpeta con nombre inválido: {student_folder}\")\n",
        "\n",
        "    def identify_speaker(self, audio_path):\n",
        "        \"\"\"\n",
        "        Identifica al hablante en un archivo de audio\n",
        "\n",
        "        Args:\n",
        "            audio_path: Ruta del archivo de audio a analizar\n",
        "\n",
        "        Returns:\n",
        "            tuple: (student_id, confidence_score) o (None, 0) si no se encuentra coincidencia\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Cargar y preprocesar audio con Resemblyzer\n",
        "            wav = preprocess_wav(audio_path)\n",
        "\n",
        "            # Extraer embedding del audio de entrada\n",
        "            input_embedding = self.encoder.embed_utterance(wav)\n",
        "\n",
        "            best_match = None\n",
        "            best_score = 0\n",
        "\n",
        "            # Comparar con todos los estudiantes registrados\n",
        "            for student_id, student_embedding in self.student_embeddings.items():\n",
        "                # Calcular similitud coseno\n",
        "                similarity = np.dot(input_embedding, student_embedding) / (\n",
        "                    np.linalg.norm(input_embedding) * np.linalg.norm(student_embedding)\n",
        "                )\n",
        "\n",
        "                if similarity > best_score and similarity > self.threshold:\n",
        "                    best_score = similarity\n",
        "                    best_match = student_id\n",
        "\n",
        "            return best_match, best_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error identificando hablante: {e}\")\n",
        "            return None, 0\n",
        "\n",
        "    def record_participation(self, student_id, confidence_score, class_date=None):\n",
        "        \"\"\"\n",
        "        Registra una participación en la base de datos\n",
        "\n",
        "        Args:\n",
        "            student_id: ID del estudiante\n",
        "            confidence_score: Puntuación de confianza\n",
        "            class_date: Fecha de la clase (por defecto hoy)\n",
        "        \"\"\"\n",
        "        if class_date is None:\n",
        "            class_date = datetime.now().date()\n",
        "\n",
        "        current_time = time.time()\n",
        "\n",
        "        # Verificar cooldown para evitar registros duplicados\n",
        "        if current_time - self.last_detection[student_id] < self.cooldown_time:\n",
        "            return False\n",
        "\n",
        "        self.last_detection[student_id] = current_time\n",
        "\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Registrar participación\n",
        "        cursor.execute('''\n",
        "            INSERT INTO participations (student_id, class_date, timestamp, confidence_score)\n",
        "            VALUES (?, ?, ?, ?)\n",
        "        ''', (student_id, class_date, datetime.now(), confidence_score))\n",
        "\n",
        "        # Actualizar contador total\n",
        "        cursor.execute('''\n",
        "            UPDATE students\n",
        "            SET total_participations = total_participations + 1\n",
        "            WHERE id = ?\n",
        "        ''', (student_id,))\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "        student_name = self.student_names.get(student_id, f\"ID_{student_id}\")\n",
        "        print(f\"✓ Participación registrada: {student_name} (Confianza: {confidence_score:.3f})\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def process_audio_stream(self, audio_path):\n",
        "        \"\"\"\n",
        "        Procesa un archivo de audio y registra participaciones\n",
        "\n",
        "        Args:\n",
        "            audio_path: Ruta del archivo de audio a procesar\n",
        "        \"\"\"\n",
        "        print(f\"Procesando audio: {audio_path}\")\n",
        "\n",
        "        student_id, confidence = self.identify_speaker(audio_path)\n",
        "\n",
        "        if student_id:\n",
        "            self.record_participation(student_id, confidence)\n",
        "        else:\n",
        "            print(\"No se identificó ningún estudiante registrado\")\n",
        "\n",
        "    def get_participation_report(self, class_date=None):\n",
        "        \"\"\"\n",
        "        Genera un reporte de participación\n",
        "\n",
        "        Args:\n",
        "            class_date: Fecha específica (por defecto hoy)\n",
        "\n",
        "        Returns:\n",
        "            list: Lista de tuplas (nombre, participaciones)\n",
        "        \"\"\"\n",
        "        if class_date is None:\n",
        "            class_date = datetime.now().date()\n",
        "\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            SELECT s.name, COUNT(p.id) as participations\n",
        "            FROM students s\n",
        "            LEFT JOIN participations p ON s.id = p.student_id\n",
        "                AND p.class_date = ?\n",
        "            GROUP BY s.id, s.name\n",
        "            ORDER BY participations DESC\n",
        "        ''', (class_date,))\n",
        "\n",
        "        results = cursor.fetchall()\n",
        "        conn.close()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_participation_stats(self, class_date=None):\n",
        "        \"\"\"Imprime estadísticas de participación\"\"\"\n",
        "        if class_date is None:\n",
        "            class_date = datetime.now().date()\n",
        "\n",
        "        print(f\"\\n=== REPORTE DE PARTICIPACIÓN - {class_date} ===\")\n",
        "\n",
        "        report = self.get_participation_report(class_date)\n",
        "\n",
        "        for name, participations in report[:10]:  # Top 10\n",
        "            print(f\"{name}: {participations} participaciones\")\n",
        "\n",
        "        total_students = len(self.student_embeddings)\n",
        "        active_students = sum(1 for _, p in report if p > 0)\n",
        "\n",
        "        print(f\"\\nResumen:\")\n",
        "        print(f\"- Estudiantes registrados: {total_students}\")\n",
        "        print(f\"- Estudiantes que participaron: {active_students}\")\n",
        "        print(f\"- Tasa de participación: {active_students/total_students*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDr0nqPCSalh",
        "outputId": "480ad1dd-1042-4703-d927-4fb418a53e49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registrando estudiante: Manuel Rua\n",
            "Estudiante Manuel Rua registrado exitosamente\n",
            "Registrando estudiante: Atzel Cervantes\n",
            "Estudiante Atzel Cervantes registrado exitosamente\n",
            "Registrando estudiante: Elmer Vela\n",
            "Estudiante Elmer Vela registrado exitosamente\n",
            "Registrando estudiante: Jorge Mendez\n",
            "Estudiante Jorge Mendez registrado exitosamente\n",
            "Registrando estudiante: Estudiante_1\n",
            "Estudiante Estudiante_1 registrado exitosamente\n",
            "Registrando estudiante: Estudiante_3\n",
            "Estudiante Estudiante_3 registrado exitosamente\n",
            "Registrando estudiante: Estudiante_2\n",
            "Estudiante Estudiante_2 registrado exitosamente\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de registro manual de estudiantes\n",
        "system.register_student(\n",
        "     student_id=5,\n",
        "     student_name=\"Manuel Rua\",\n",
        "     audio_samples=[\"/content/drive/MyDrive/Manuel.wav\"]\n",
        ")\n",
        "\n",
        "system.register_student(\n",
        "     student_id=6,\n",
        "     student_name=\"Atzel Cervantes\",\n",
        "     audio_samples=[\"/content/drive/MyDrive/Atzel.wav\"]\n",
        ")\n",
        "\n",
        "system.register_student(\n",
        "     student_id=7,\n",
        "     student_name=\"Elmer Vela\",\n",
        "     audio_samples=[\"/content/drive/MyDrive/Pirlo.wav\"]\n",
        ")\n",
        "\n",
        "system.register_student(\n",
        "     student_id=8,\n",
        "     student_name=\"Jorge Mendez\",\n",
        "     audio_samples=[\"/content/drive/MyDrive/Mendes.wav\"]\n",
        ")\n",
        "\n",
        "# Cargar estudiantes desde directorio (método recomendado)\n",
        "system.load_students_from_directory(\"/content/drive/MyDrive/students_data\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9xh3lQJdIy4",
        "outputId": "ee28d20b-8fa6-4678-9f6e-3d61c750fdcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesando audio: /content/drive/MyDrive/Manco.wav\n",
            "\n",
            "=== REPORTE DE PARTICIPACIÓN - 2025-06-28 ===\n",
            "Estudiante_1: 2 participaciones\n",
            "Manuel Rua: 2 participaciones\n",
            "Elmer Vela: 1 participaciones\n",
            "Atzel Cervantes: 0 participaciones\n",
            "Estudiante_2: 0 participaciones\n",
            "Estudiante_3: 0 participaciones\n",
            "Jorge Mendez: 0 participaciones\n",
            "\n",
            "Resumen:\n",
            "- Estudiantes registrados: 7\n",
            "- Estudiantes que participaron: 3\n",
            "- Tasa de participación: 42.9%\n",
            "Sistema de participación inicializado correctamente\n"
          ]
        }
      ],
      "source": [
        "# Procesar audio en tiempo real\n",
        "system.process_audio_stream(\"/content/drive/MyDrive/Manco.wav\")\n",
        "\n",
        "# Generar reporte\n",
        "system.print_participation_stats()\n",
        "\n",
        "print(\"Sistema de participación inicializado correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_mIVgFEn9rZ"
      },
      "source": [
        "# **Whisper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD_iTvukoAZI",
        "outputId": "ea86fb39-23fe-4cde-b3d6-2976094e17a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!apt-get install -y -qq ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF-xoTEEoIaR"
      },
      "outputs": [],
      "source": [
        "## 2) Importar y cargar el modelo Whisper\n",
        "import whisper\n",
        "model = whisper.load_model(\"base\")  # o \"small\", \"medium\", \"large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBZhdALAoNWj"
      },
      "outputs": [],
      "source": [
        "def _bytes_from_base64(data_url):\n",
        "    header, b64 = data_url.split(\",\", 1)\n",
        "    return base64.b64decode(b64)\n",
        "\n",
        "def process_audio(data_url):\n",
        "    # 1) decodifica y guarda el chunk .webm\n",
        "    webm_bytes = _bytes_from_base64(data_url)\n",
        "    tmp_webm = tempfile.NamedTemporaryFile(suffix=\".webm\", delete=False)\n",
        "    tmp_webm.write(webm_bytes)\n",
        "    tmp_webm.flush()\n",
        "    tmp_webm.close()\n",
        "\n",
        "    # 2) convierte a WAV 16 kHz mono\n",
        "    tmp_wav = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
        "    tmp_wav.close()\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\n",
        "        \"-y\",                    # sobrescribe si ya existe\n",
        "        \"-i\", tmp_webm.name,     # input\n",
        "        \"-ar\", \"16000\",          # sample rate\n",
        "        \"-ac\", \"1\",              # canales mono\n",
        "        tmp_wav.name\n",
        "    ]\n",
        "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
        "\n",
        "    # 3) transcribe con Whisper\n",
        "    result = model.transcribe(tmp_wav.name, fp16=False)\n",
        "    print(result[\"text\"].strip())\n",
        "\n",
        "    # 4) limpia archivos temporales\n",
        "    os.remove(tmp_webm.name)\n",
        "    os.remove(tmp_wav.name)\n",
        "\n",
        "# registra el callback\n",
        "output.register_callback('notebook.AudioCallback', process_audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9r4iR3voVjD"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Javascript, display\n",
        "\n",
        "def start_recording(chunk_ms=3000):\n",
        "    js = f\"\"\"\n",
        "    (() => {{\n",
        "      const rec_len = {chunk_ms};\n",
        "      navigator.mediaDevices.getUserMedia({{ audio: true }})\n",
        "        .then(stream => {{\n",
        "          const recorder = new MediaRecorder(stream);\n",
        "          recorder.ondataavailable = async e => {{\n",
        "            const reader = new FileReader();\n",
        "            reader.readAsDataURL(e.data);\n",
        "            reader.onloadend = () => {{\n",
        "              const base64data = reader.result;\n",
        "              google.colab.kernel.invokeFunction('notebook.AudioCallback', [base64data], {{}});\n",
        "            }};\n",
        "          }};\n",
        "          recorder.start(rec_len);\n",
        "          recorder.onstop = () => recorder.start(rec_len);\n",
        "        }})\n",
        "        .catch(err => alert('Error accediendo al micrófono: ' + err));\n",
        "    }})();\n",
        "    \"\"\"\n",
        "    display(Javascript(js))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "XxUxRZ9ioaVY",
        "outputId": "01794281-4034-4d3e-cd35-3dc46a5b967d"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    (() => {\n",
              "      const rec_len = 3000;\n",
              "      navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "        .then(stream => {\n",
              "          const recorder = new MediaRecorder(stream);\n",
              "          recorder.ondataavailable = async e => {\n",
              "            const reader = new FileReader();\n",
              "            reader.readAsDataURL(e.data);\n",
              "            reader.onloadend = () => {\n",
              "              const base64data = reader.result;\n",
              "              google.colab.kernel.invokeFunction('notebook.AudioCallback', [base64data], {});\n",
              "            };\n",
              "          };\n",
              "          recorder.start(rec_len);\n",
              "          recorder.onstop = () => recorder.start(rec_len);\n",
              "        })\n",
              "        .catch(err => alert('Error accediendo al micrófono: ' + err));\n",
              "    })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dispositivos de audio disponibles:\n",
            "Error al acceder al micrófono: [Errno -9996] Invalid input device (no default output device)\n",
            "Intenta ejecutar en tu navegador y permitir acceso al micrófono\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ejecutar\n",
        "start_recording(chunk_ms=3000)  # aquí defines la longitud de cada fragmento en ms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ-Oik-hppDY",
        "outputId": "cb492d01-e12c-468f-8f51-2fdc4ee50614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20250625)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Requirement already satisfied: pyaudio in /usr/local/lib/python3.11/dist-packages (0.2.14)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "portaudio19-dev is already the newest version (19.6.0-1.1).\n",
            "Suggested packages:\n",
            "  python-pyaudio-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pyaudio\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 25.9 kB of archives.\n",
            "After this operation, 117 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-pyaudio amd64 0.2.11-1.3ubuntu1 [25.9 kB]\n",
            "Fetched 25.9 kB in 0s (112 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3-pyaudio.\n",
            "(Reading database ... 126352 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-pyaudio_0.2.11-1.3ubuntu1_amd64.deb ...\n",
            "Unpacking python3-pyaudio (0.2.11-1.3ubuntu1) ...\n",
            "Setting up python3-pyaudio (0.2.11-1.3ubuntu1) ...\n",
            "Cargando modelo Whisper...\n",
            "Modelo cargado exitosamente!\n",
            "============================================================\n",
            "🎙️  WHISPER TRANSCRIPCIÓN EN TIEMPO REAL\n",
            "============================================================\n",
            "\n",
            "📋 INSTRUCCIONES:\n",
            "1. Ejecuta start_recording() para comenzar\n",
            "2. Habla cerca del micrófono\n",
            "3. Ejecuta stop_recording() para detener\n",
            "\n",
            "⚙️  CONFIGURACIÓN ACTUAL:\n",
            "   • Modelo: Whisper\n",
            "   • Idioma: Español (cambia en process_audio si necesitas otro)\n",
            "   • Duración de segmentos: 3 segundos\n",
            "   • Frecuencia de muestreo: 16000 Hz\n",
            "\n",
            "🔧 COMANDOS:\n",
            "   • Para iniciar: start_recording()\n",
            "   • Para detener: stop_recording()\n",
            "\n",
            "✅ Todo listo! Ejecuta start_recording() cuando quieras comenzar.\n"
          ]
        }
      ],
      "source": [
        "# Código para transcripción en tiempo real con Whisper en Google Colab\n",
        "# Ejecuta cada celda en orden\n",
        "\n",
        "# CELDA 1: Instalación de dependencias\n",
        "!pip install openai-whisper\n",
        "!pip install pyaudio\n",
        "!sudo apt-get install portaudio19-dev python3-pyaudio\n",
        "\n",
        "# CELDA 2: Imports y configuración\n",
        "import whisper\n",
        "import pyaudio\n",
        "import wave\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import io\n",
        "\n",
        "# CELDA 3: Configuración del audio\n",
        "CHUNK = 1024\n",
        "FORMAT = pyaudio.paInt16\n",
        "CHANNELS = 1\n",
        "RATE = 16000\n",
        "RECORD_SECONDS = 3  # Duración de cada segmento a transcribir\n",
        "\n",
        "# CELDA 4: Cargar modelo Whisper\n",
        "print(\"Cargando modelo Whisper...\")\n",
        "model = whisper.load_model(\"base\")  # Puedes usar \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
        "print(\"Modelo cargado exitosamente!\")\n",
        "\n",
        "# CELDA 5: Clase para manejo de audio en tiempo real\n",
        "class RealTimeTranscriber:\n",
        "    def __init__(self):\n",
        "        self.audio_queue = queue.Queue()\n",
        "        self.is_recording = False\n",
        "        self.transcription_text = \"\"\n",
        "\n",
        "    def audio_callback(self, in_data, frame_count, time_info, status):\n",
        "        \"\"\"Callback para capturar audio\"\"\"\n",
        "        self.audio_queue.put(in_data)\n",
        "        return (in_data, pyaudio.paContinue)\n",
        "\n",
        "    def record_audio(self):\n",
        "        \"\"\"Función para grabar audio continuamente\"\"\"\n",
        "        p = pyaudio.PyAudio()\n",
        "\n",
        "        # Verificar dispositivos de audio disponibles\n",
        "        print(\"Dispositivos de audio disponibles:\")\n",
        "        for i in range(p.get_device_count()):\n",
        "            info = p.get_device_info_by_index(i)\n",
        "            print(f\"{i}: {info['name']} - Canales de entrada: {info['maxInputChannels']}\")\n",
        "\n",
        "        try:\n",
        "            stream = p.open(format=FORMAT,\n",
        "                          channels=CHANNELS,\n",
        "                          rate=RATE,\n",
        "                          input=True,\n",
        "                          frames_per_buffer=CHUNK,\n",
        "                          stream_callback=self.audio_callback)\n",
        "\n",
        "            stream.start_stream()\n",
        "            print(\"🎤 Grabación iniciada. Habla ahora...\")\n",
        "\n",
        "            while self.is_recording:\n",
        "                time.sleep(0.1)\n",
        "\n",
        "            stream.stop_stream()\n",
        "            stream.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al acceder al micrófono: {e}\")\n",
        "            print(\"Intenta ejecutar en tu navegador y permitir acceso al micrófono\")\n",
        "        finally:\n",
        "            p.terminate()\n",
        "\n",
        "    def process_audio(self):\n",
        "        \"\"\"Procesar audio y generar transcripciones\"\"\"\n",
        "        audio_buffer = []\n",
        "\n",
        "        while self.is_recording:\n",
        "            try:\n",
        "                # Recopilar audio por segmentos\n",
        "                if not self.audio_queue.empty():\n",
        "                    data = self.audio_queue.get()\n",
        "                    audio_buffer.append(data)\n",
        "\n",
        "                    # Cuando tengamos suficiente audio (aprox 3 segundos)\n",
        "                    if len(audio_buffer) >= (RATE * RECORD_SECONDS) // CHUNK:\n",
        "                        # Convertir a numpy array\n",
        "                        audio_data = b''.join(audio_buffer)\n",
        "                        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
        "\n",
        "                        # Transcribir con Whisper\n",
        "                        try:\n",
        "                            result = model.transcribe(audio_np, language='es')  # Cambia 'es' por tu idioma\n",
        "                            text = result['text'].strip()\n",
        "\n",
        "                            if text:  # Solo mostrar si hay texto\n",
        "                                self.transcription_text += f\"[{time.strftime('%H:%M:%S')}] {text}\\n\"\n",
        "                                self.update_display()\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error en transcripción: {e}\")\n",
        "\n",
        "                        # Limpiar buffer (mantener overlap)\n",
        "                        overlap = len(audio_buffer) // 4\n",
        "                        audio_buffer = audio_buffer[-overlap:]\n",
        "\n",
        "                time.sleep(0.1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error procesando audio: {e}\")\n",
        "                break\n",
        "\n",
        "    def update_display(self):\n",
        "        \"\"\"Actualizar la visualización de transcripciones\"\"\"\n",
        "        clear_output(wait=True)\n",
        "        html_content = f\"\"\"\n",
        "        <div style=\"background-color: #f0f0f0; padding: 20px; border-radius: 10px;\n",
        "                    border: 2px solid #4CAF50; max-height: 400px; overflow-y: auto;\">\n",
        "            <h3 style=\"color: #2E7D32; margin-top: 0;\">🎤 Transcripción en Tiempo Real</h3>\n",
        "            <div style=\"font-family: monospace; white-space: pre-wrap; background-color: white;\n",
        "                        padding: 15px; border-radius: 5px; border-left: 4px solid #4CAF50;\">\n",
        "{self.transcription_text}\n",
        "            </div>\n",
        "            <p style=\"color: #666; font-size: 12px; margin-bottom: 0;\">\n",
        "                💡 Presiona el botón \"Detener\" para finalizar la transcripción\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(html_content))\n",
        "\n",
        "    def start_transcription(self):\n",
        "        \"\"\"Iniciar transcripción en tiempo real\"\"\"\n",
        "        self.is_recording = True\n",
        "        self.transcription_text = \"\"\n",
        "\n",
        "        # Iniciar hilos para grabación y procesamiento\n",
        "        record_thread = threading.Thread(target=self.record_audio)\n",
        "        process_thread = threading.Thread(target=self.process_audio)\n",
        "\n",
        "        record_thread.daemon = True\n",
        "        process_thread.daemon = True\n",
        "\n",
        "        record_thread.start()\n",
        "        process_thread.start()\n",
        "\n",
        "        return record_thread, process_thread\n",
        "\n",
        "    def stop_transcription(self):\n",
        "        \"\"\"Detener transcripción\"\"\"\n",
        "        self.is_recording = False\n",
        "        print(\"🛑 Transcripción detenida.\")\n",
        "\n",
        "# CELDA 6: Inicializar transcriptor\n",
        "transcriber = RealTimeTranscriber()\n",
        "\n",
        "# CELDA 7: Funciones de control\n",
        "def start_recording():\n",
        "    \"\"\"Iniciar grabación y transcripción\"\"\"\n",
        "    print(\"🚀 Iniciando transcripción en tiempo real...\")\n",
        "    threads = transcriber.start_transcription()\n",
        "    return threads\n",
        "\n",
        "def stop_recording():\n",
        "    \"\"\"Detener grabación y transcripción\"\"\"\n",
        "    transcriber.stop_transcription()\n",
        "\n",
        "# CELDA 8: Controles principales\n",
        "print(\"=\" * 60)\n",
        "print(\"🎙️  WHISPER TRANSCRIPCIÓN EN TIEMPO REAL\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"📋 INSTRUCCIONES:\")\n",
        "print(\"1. Ejecuta start_recording() para comenzar\")\n",
        "print(\"2. Habla cerca del micrófono\")\n",
        "print(\"3. Ejecuta stop_recording() para detener\")\n",
        "print()\n",
        "print(\"⚙️  CONFIGURACIÓN ACTUAL:\")\n",
        "print(f\"   • Modelo: {model.__class__.__name__}\")\n",
        "print(f\"   • Idioma: Español (cambia en process_audio si necesitas otro)\")\n",
        "print(f\"   • Duración de segmentos: {RECORD_SECONDS} segundos\")\n",
        "print(f\"   • Frecuencia de muestreo: {RATE} Hz\")\n",
        "print()\n",
        "print(\"🔧 COMANDOS:\")\n",
        "print(\"   • Para iniciar: start_recording()\")\n",
        "print(\"   • Para detener: stop_recording()\")\n",
        "print()\n",
        "\n",
        "# CELDA 9: Ejemplo de uso\n",
        "# Descomenta las siguientes líneas para iniciar automáticamente:\n",
        "# print(\"Iniciando en 3 segundos...\")\n",
        "# time.sleep(3)\n",
        "# threads = start_recording()\n",
        "\n",
        "print(\"✅ Todo listo! Ejecuta start_recording() cuando quieras comenzar.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i38SQGfjpuqE",
        "outputId": "92e09539-8a41-462f-a08c-b9664da76d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Iniciando transcripción en tiempo real...\n"
          ]
        }
      ],
      "source": [
        "# Para iniciar la transcripción:\n",
        "threads = start_recording()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1YTSY-_pyHX",
        "outputId": "ce12dd07-fb76-43f3-edc8-d5da8c884c86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🛑 Transcripción detenida.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Para detener:\n",
        "stop_recording()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qeQcinw-qQHk",
        "outputId": "82bcec8e-ae34-43c0-f17a-788c9024f936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20250625)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "🔄 Cargando modelo Whisper...\n",
            "✅ Modelo Whisper cargado!\n",
            "🎯 Creando interfaz web...\n",
            "🚀 Iniciando interfaz web...\n",
            "============================================================\n",
            "🎉 ¡INTERFAZ LISTA!\n",
            "📱 Se abrirá una ventana web donde podrás:\n",
            "   • Grabar con el micrófono\n",
            "   • Ver transcripciones en tiempo real\n",
            "   • Copiar el texto transcrito\n",
            "============================================================\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0f6ec59f08be6f45a1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://0f6ec59f08be6f45a1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 VERSIÓN ALTERNATIVA SIMPLE\n",
            "==================================================\n",
            "✅ Todo configurado!\n",
            "\n",
            "🔧 Si tienes problemas:\n",
            "1. Usa la interfaz web que se abrió arriba\n",
            "2. O ejecuta la versión simple al final\n",
            "3. Asegúrate de permitir acceso al micrófono en el navegador\n"
          ]
        }
      ],
      "source": [
        "# SOLUCIÓN PARA WHISPER EN COLAB - Grabación por chunks con interfaz web\n",
        "# Ejecuta cada celda en orden\n",
        "\n",
        "# CELDA 1: Instalación de dependencias\n",
        "!pip install openai-whisper\n",
        "!pip install gradio\n",
        "!apt-get install ffmpeg\n",
        "\n",
        "# CELDA 2: Imports\n",
        "import whisper\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# CELDA 3: Cargar modelo Whisper\n",
        "print(\"🔄 Cargando modelo Whisper...\")\n",
        "model = whisper.load_model(\"base\")  # Cambia por \"small\", \"medium\", \"large\" si quieres más precisión\n",
        "print(\"✅ Modelo Whisper cargado!\")\n",
        "\n",
        "# CELDA 4: Variables globales para transcripción continua\n",
        "transcription_history = []\n",
        "is_running = False\n",
        "\n",
        "def transcribe_audio(audio_file):\n",
        "    \"\"\"Transcribir archivo de audio individual\"\"\"\n",
        "    if audio_file is None:\n",
        "        return \"❌ No se detectó audio\"\n",
        "\n",
        "    try:\n",
        "        # Transcribir con Whisper\n",
        "        result = model.transcribe(audio_file, language='es')  # Cambia 'es' por tu idioma\n",
        "        text = result['text'].strip()\n",
        "\n",
        "        if text:\n",
        "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "            transcription_entry = f\"[{timestamp}] {text}\"\n",
        "            transcription_history.append(transcription_entry)\n",
        "\n",
        "            # Mantener solo las últimas 50 transcripciones\n",
        "            if len(transcription_history) > 50:\n",
        "                transcription_history.pop(0)\n",
        "\n",
        "            return \"\\n\".join(transcription_history)\n",
        "        else:\n",
        "            return \"\\n\".join(transcription_history) + \"\\n⚠️ No se detectó voz clara\"\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Error: {str(e)}\"\n",
        "        transcription_history.append(error_msg)\n",
        "        return \"\\n\".join(transcription_history)\n",
        "\n",
        "def clear_history():\n",
        "    \"\"\"Limpiar historial de transcripciones\"\"\"\n",
        "    global transcription_history\n",
        "    transcription_history = []\n",
        "    return \"🧹 Historial limpiado - Listo para nuevas transcripciones\"\n",
        "\n",
        "# CELDA 5: Crear interfaz con Gradio\n",
        "print(\"🎯 Creando interfaz web...\")\n",
        "\n",
        "# Interfaz principal\n",
        "with gr.Blocks(title=\"🎤 Whisper Transcripción\", theme=gr.themes.Soft()) as interface:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # 🎤 Whisper - Transcripción de Voz en Tiempo Real\n",
        "\n",
        "    ## 📋 Instrucciones:\n",
        "    1. **Haz clic en el micrófono** 🎙️ para empezar a grabar\n",
        "    2. **Habla claramente** durante 3-10 segundos\n",
        "    3. **Detén la grabación** y espera la transcripción\n",
        "    4. **Repite** para continuar transcribiendo\n",
        "\n",
        "    ### 💡 Consejos:\n",
        "    - Habla cerca del micrófono\n",
        "    - Evita ruido de fondo\n",
        "    - Grabaciones de 5-10 segundos funcionan mejor\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            # Input de audio con grabación\n",
        "            audio_input = gr.Audio(\n",
        "                sources=[\"microphone\"],\n",
        "                type=\"filepath\",\n",
        "                label=\"🎙️ Graba tu voz aquí\",\n",
        "                show_download_button=False\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                transcribe_btn = gr.Button(\"🔄 Transcribir\", variant=\"primary\", size=\"lg\")\n",
        "                clear_btn = gr.Button(\"🧹 Limpiar Historial\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            # Output de transcripción\n",
        "            output_text = gr.Textbox(\n",
        "                label=\"📝 Transcripciones\",\n",
        "                lines=15,\n",
        "                max_lines=20,\n",
        "                placeholder=\"Las transcripciones aparecerán aquí...\",\n",
        "                show_copy_button=True\n",
        "            )\n",
        "\n",
        "    # Información adicional\n",
        "    gr.Markdown(\"\"\"\n",
        "    ### ⚙️ Configuración actual:\n",
        "    - **Modelo**: Whisper Base (buen balance velocidad/precisión)\n",
        "    - **Idioma**: Español (configurable)\n",
        "    - **Formato**: Automático\n",
        "\n",
        "    ### 🔧 Para cambiar idioma:\n",
        "    Modifica `language='es'` en el código por:\n",
        "    - `'en'` para inglés\n",
        "    - `'fr'` para francés\n",
        "    - `'de'` para alemán\n",
        "    - `None` para detección automática\n",
        "    \"\"\")\n",
        "\n",
        "    # Eventos\n",
        "    transcribe_btn.click(\n",
        "        fn=transcribe_audio,\n",
        "        inputs=[audio_input],\n",
        "        outputs=[output_text]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_history,\n",
        "        outputs=[output_text]\n",
        "    )\n",
        "\n",
        "    # Auto-transcribir cuando se grabe algo\n",
        "    audio_input.change(\n",
        "        fn=transcribe_audio,\n",
        "        inputs=[audio_input],\n",
        "        outputs=[output_text]\n",
        "    )\n",
        "\n",
        "# CELDA 6: Lanzar la interfaz\n",
        "print(\"🚀 Iniciando interfaz web...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"🎉 ¡INTERFAZ LISTA!\")\n",
        "print(\"📱 Se abrirá una ventana web donde podrás:\")\n",
        "print(\"   • Grabar con el micrófono\")\n",
        "print(\"   • Ver transcripciones en tiempo real\")\n",
        "print(\"   • Copiar el texto transcrito\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Lanzar con configuración optimizada para Colab\n",
        "interface.launch(\n",
        "    share=True,          # Crear enlace público\n",
        "    debug=False,         # Sin debug para mejor rendimiento\n",
        "    server_name=\"0.0.0.0\",  # Accesible desde cualquier IP\n",
        "    server_port=7860,    # Puerto estándar\n",
        "    show_error=True,     # Mostrar errores\n",
        "    quiet=False          # Mostrar logs\n",
        ")\n",
        "\n",
        "# CELDA 7: Versión simplificada si la anterior no funciona\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔄 VERSIÓN ALTERNATIVA SIMPLE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def simple_transcribe():\n",
        "    \"\"\"Versión simple - un archivo a la vez\"\"\"\n",
        "    print(\"📁 Sube un archivo de audio o usa la grabadora web:\")\n",
        "\n",
        "    def process_file(audio_file):\n",
        "        if audio_file is None:\n",
        "            return \"Sube un archivo de audio\"\n",
        "\n",
        "        result = model.transcribe(audio_file, language='es')\n",
        "        return result['text']\n",
        "\n",
        "    simple_interface = gr.Interface(\n",
        "        fn=process_file,\n",
        "        inputs=gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\"),\n",
        "        outputs=gr.Textbox(label=\"Transcripción\", lines=5),\n",
        "        title=\"🎤 Whisper Simple\",\n",
        "        description=\"Graba o sube audio para transcribir\"\n",
        "    )\n",
        "\n",
        "    return simple_interface\n",
        "\n",
        "# Para usar la versión simple, ejecuta:\n",
        "# simple_interface = simple_transcribe()\n",
        "# simple_interface.launch(share=True)\n",
        "\n",
        "print(\"✅ Todo configurado!\")\n",
        "print(\"\\n🔧 Si tienes problemas:\")\n",
        "print(\"1. Usa la interfaz web que se abrió arriba\")\n",
        "print(\"2. O ejecuta la versión simple al final\")\n",
        "print(\"3. Asegúrate de permitir acceso al micrófono en el navegador\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "vNV7OJWWqVH1",
        "outputId": "2bf8724c-6725-43f1-e120-5683c6c83613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📁 Sube un archivo de audio o usa la grabadora web:\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f40fc0708f1b7f00d6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://f40fc0708f1b7f00d6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_interface = simple_transcribe()\n",
        "simple_interface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4fjM5IxrMEe",
        "outputId": "d38a4067-a5e0-44a7-b8ca-08c402d05708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Cargando Whisper...\n",
            "✅ Whisper listo!\n",
            "\n",
            "🎯 FUNCIONES DISPONIBLES:\n",
            "========================================\n",
            "1️⃣  grabar_y_transcribir(5)     # Graba 5 segundos\n",
            "2️⃣  transcribir_continuo()      # Modo múltiples grabaciones\n",
            "========================================\n",
            "\n",
            "💡 EJEMPLOS DE USO:\n",
            "   grabar_y_transcribir(3)   # Grabación corta\n",
            "   grabar_y_transcribir(10)  # Grabación larga\n",
            "   transcribir_continuo()    # Sesión completa\n",
            "\n",
            "✅ ¡Todo listo! Usa las funciones arriba\n"
          ]
        }
      ],
      "source": [
        "# WHISPER SIMPLE PARA COLAB - Solo Terminal\n",
        "# Ejecuta esta celda y luego usa las funciones\n",
        "\n",
        "!pip install openai-whisper -q\n",
        "\n",
        "import whisper\n",
        "import base64\n",
        "from IPython.display import HTML, display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import io\n",
        "import wave\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Cargar modelo\n",
        "print(\"🔄 Cargando Whisper...\")\n",
        "model = whisper.load_model(\"base\")\n",
        "print(\"✅ Whisper listo!\")\n",
        "\n",
        "def grabar_y_transcribir(segundos=5):\n",
        "    \"\"\"\n",
        "    Graba audio del micrófono y lo transcribe\n",
        "    \"\"\"\n",
        "    print(f\"🎙️ Preparando grabación de {segundos} segundos...\")\n",
        "\n",
        "    # JavaScript para grabación\n",
        "    js_code = f\"\"\"\n",
        "    const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "\n",
        "    const record = async (time) => {{\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({{ audio: true }})\n",
        "        const recorder = new MediaRecorder(stream)\n",
        "        const chunks = []\n",
        "\n",
        "        recorder.ondataavailable = e => chunks.push(e.data)\n",
        "\n",
        "        const audioPromise = new Promise(resolve => {{\n",
        "            recorder.onstop = () => {{\n",
        "                const blob = new Blob(chunks, {{ type: 'audio/wav' }})\n",
        "                const reader = new FileReader()\n",
        "                reader.onloadend = () => resolve(reader.result.split(',')[1])\n",
        "                reader.readAsDataURL(blob)\n",
        "            }}\n",
        "        }})\n",
        "\n",
        "        recorder.start()\n",
        "        console.log('🔴 Grabando...')\n",
        "        await sleep(time)\n",
        "        recorder.stop()\n",
        "        stream.getTracks().forEach(track => track.stop())\n",
        "\n",
        "        return await audioPromise\n",
        "    }}\n",
        "\n",
        "    record({segundos * 1000})\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"🔴 Grabando {segundos} segundos... ¡HABLA AHORA!\")\n",
        "\n",
        "    try:\n",
        "        # Ejecutar JavaScript y obtener audio\n",
        "        audio_data = eval_js(js_code)\n",
        "\n",
        "        if audio_data:\n",
        "            # Decodificar y procesar audio\n",
        "            audio_bytes = base64.b64decode(audio_data)\n",
        "\n",
        "            # Crear archivo temporal\n",
        "            with open('/tmp/audio.wav', 'wb') as f:\n",
        "                f.write(audio_bytes)\n",
        "\n",
        "            print(\"✅ Audio capturado, transcribiendo...\")\n",
        "\n",
        "            # Transcribir con Whisper\n",
        "            result = model.transcribe('/tmp/audio.wav', language='es')\n",
        "\n",
        "            # Mostrar resultado\n",
        "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "            text = result['text'].strip()\n",
        "\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(f\"📝 TRANSCRIPCIÓN [{timestamp}]:\")\n",
        "            print(\"=\"*50)\n",
        "            print(f\"🗣️  {text}\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            return text\n",
        "        else:\n",
        "            print(\"❌ No se pudo capturar audio\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        print(\"💡 Asegúrate de permitir acceso al micrófono\")\n",
        "        return None\n",
        "\n",
        "def transcribir_continuo():\n",
        "    \"\"\"\n",
        "    Modo continuo - transcribe múltiples grabaciones\n",
        "    \"\"\"\n",
        "    print(\"\\n🔄 MODO CONTINUO ACTIVADO\")\n",
        "    print(\"=\"*40)\n",
        "    print(\"🎯 Instrucciones:\")\n",
        "    print(\"  • Cada grabación dura 5 segundos\")\n",
        "    print(\"  • Habla cuando veas 'HABLA AHORA!'\")\n",
        "    print(\"  • Presiona Enter para continuar\")\n",
        "    print(\"  • Escribe 'q' para salir\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    transcripciones = []\n",
        "    contador = 1\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\n📍 Grabación #{contador}\")\n",
        "        input(\"⏸️  Presiona Enter para grabar (o 'q' para salir): \")\n",
        "\n",
        "        if input == 'q':\n",
        "            break\n",
        "\n",
        "        texto = grabar_y_transcribir(5)\n",
        "        if texto:\n",
        "            transcripciones.append(f\"[{contador}] {texto}\")\n",
        "\n",
        "        contador += 1\n",
        "\n",
        "        # Mostrar todas las transcripciones\n",
        "        if transcripciones:\n",
        "            print(f\"\\n📋 HISTORIAL ({len(transcripciones)} grabaciones):\")\n",
        "            print(\"-\" * 40)\n",
        "            for t in transcripciones[-5:]:  # Últimas 5\n",
        "                print(f\"  {t}\")\n",
        "            if len(transcripciones) > 5:\n",
        "                print(f\"  ... y {len(transcripciones)-5} más\")\n",
        "\n",
        "    print(\"✅ Modo continuo terminado\")\n",
        "    return transcripciones\n",
        "\n",
        "# FUNCIONES LISTAS PARA USAR:\n",
        "print(\"\\n🎯 FUNCIONES DISPONIBLES:\")\n",
        "print(\"=\"*40)\n",
        "print(\"1️⃣  grabar_y_transcribir(5)     # Graba 5 segundos\")\n",
        "print(\"2️⃣  transcribir_continuo()      # Modo múltiples grabaciones\")\n",
        "print(\"=\"*40)\n",
        "print()\n",
        "print(\"💡 EJEMPLOS DE USO:\")\n",
        "print(\"   grabar_y_transcribir(3)   # Grabación corta\")\n",
        "print(\"   grabar_y_transcribir(10)  # Grabación larga\")\n",
        "print(\"   transcribir_continuo()    # Sesión completa\")\n",
        "print()\n",
        "print(\"✅ ¡Todo listo! Usa las funciones arriba\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fVgPPxttt2wJ",
        "outputId": "031dd2bd-c2af-47d7-9df9-827d710feddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔄 MODO CONTINUO ACTIVADO\n",
            "========================================\n",
            "🎯 Instrucciones:\n",
            "  • Cada grabación dura 5 segundos\n",
            "  • Habla cuando veas 'HABLA AHORA!'\n",
            "  • Presiona Enter para continuar\n",
            "  • Escribe 'q' para salir\n",
            "========================================\n",
            "\n",
            "📍 Grabación #1\n",
            "🎙️ Preparando grabación de 5 segundos...\n",
            "🔴 Grabando 5 segundos... ¡HABLA AHORA!\n",
            "✅ Audio capturado, transcribiendo...\n",
            "\n",
            "==================================================\n",
            "📝 TRANSCRIPCIÓN [08:46:40]:\n",
            "==================================================\n",
            "🗣️  Bueno, empezar a transcribir, por favor transcriba.\n",
            "==================================================\n",
            "\n",
            "📋 HISTORIAL (1 grabaciones):\n",
            "----------------------------------------\n",
            "  [1] Bueno, empezar a transcribir, por favor transcriba.\n",
            "\n",
            "📍 Grabación #2\n",
            "🎙️ Preparando grabación de 5 segundos...\n",
            "🔴 Grabando 5 segundos... ¡HABLA AHORA!\n",
            "✅ Audio capturado, transcribiendo...\n",
            "\n",
            "==================================================\n",
            "📝 TRANSCRIPCIÓN [08:46:55]:\n",
            "==================================================\n",
            "🗣️  buenas noches empiezas a transmitir el video y la duración de la\n",
            "==================================================\n",
            "\n",
            "📋 HISTORIAL (2 grabaciones):\n",
            "----------------------------------------\n",
            "  [1] Bueno, empezar a transcribir, por favor transcriba.\n",
            "  [2] buenas noches empiezas a transmitir el video y la duración de la\n",
            "\n",
            "📍 Grabación #3\n",
            "🎙️ Preparando grabación de 5 segundos...\n",
            "🔴 Grabando 5 segundos... ¡HABLA AHORA!\n",
            "✅ Audio capturado, transcribiendo...\n",
            "\n",
            "==================================================\n",
            "📝 TRANSCRIPCIÓN [08:47:34]:\n",
            "==================================================\n",
            "🗣️  Allí� es,ere siempre Allíé Ma Yין Acá Lora Y cave La Y\n",
            "==================================================\n",
            "\n",
            "📋 HISTORIAL (3 grabaciones):\n",
            "----------------------------------------\n",
            "  [1] Bueno, empezar a transcribir, por favor transcriba.\n",
            "  [2] buenas noches empiezas a transmitir el video y la duración de la\n",
            "  [3] Allí� es,ere siempre Allíé Ma Yין Acá Lora Y cave La Y\n",
            "\n",
            "📍 Grabación #4\n"
          ]
        }
      ],
      "source": [
        "# Modo continuo (múltiples grabaciones)\n",
        "transcribir_continuo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "Xv8X_I8cr1Vl",
        "outputId": "5483676d-5039-4908-c54b-7a2eb64df5c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Cargando Whisper...\n",
            "✅ Whisper cargado!\n",
            "\n",
            "📱 Dispositivos de audio:\n",
            "❌ Error con micrófono: [Errno -9996] Invalid input device (no default output device)\n",
            "\n",
            "💡 Soluciones:\n",
            "1. Ejecuta en tu máquina local (no Colab)\n",
            "2. O usa la versión alternativa abajo\n",
            "✅ Whisper terminado\n",
            "\n",
            "⚠️  La versión de terminal completa no funciona en Colab\n",
            "🔄 Cambiando a versión manual...\n",
            "\n",
            "🔄 VERSIÓN COLAB (grabación manual)\n",
            "========================================\n",
            "🎙️ Usa esta función para grabar:\n",
            "record_audio(5)  # Graba 5 segundos\n",
            "\n",
            "🧪 Ejecutando grabación de prueba...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <button id=\"record\">🎤 Grabar 3s</button>\n",
              "        <script>\n",
              "        \n",
              "    const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "    const b2text = blob => new Promise(resolve => {\n",
              "      const reader = new FileReader()\n",
              "      reader.onloadend = e => resolve(e.srcElement.result)\n",
              "      reader.readAsDataURL(blob)\n",
              "    })\n",
              "\n",
              "    var record = time => new Promise(async resolve => {\n",
              "      stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "      recorder = new MediaRecorder(stream)\n",
              "      chunks = []\n",
              "      recorder.ondataavailable = e => chunks.push(e.data)\n",
              "      recorder.onstop = async ()=>{\n",
              "        blob = new Blob(chunks)\n",
              "        text = await b2text(blob)\n",
              "        resolve(text)\n",
              "      }\n",
              "      recorder.start()\n",
              "      await sleep(time)\n",
              "      recorder.stop()\n",
              "    })\n",
              "    \n",
              "        document.querySelector(\"#record\").onclick = async () => {\n",
              "            document.querySelector(\"#record\").innerHTML = \"🔴 Grabando...\"\n",
              "            audio = await record(3000)\n",
              "            document.querySelector(\"#record\").innerHTML = \"✅ Listo\"\n",
              "            google.colab.kernel.invokeFunction('notebook.save_audio', [audio], {})\n",
              "        }\n",
              "        </script>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Intentar versión completa primero\n",
        "whisper_rt = RealTimeWhisper()\n",
        "whisper_rt.start()\n",
        "print(\"\\n⚠️  La versión de terminal completa no funciona en Colab\")\n",
        "print(\"🔄 Cambiando a versión manual...\")\n",
        "record_func = colab_version()\n",
        "\n",
        "# Auto-ejecutar una grabación de prueba\n",
        "print(\"\\n🧪 Ejecutando grabación de prueba...\")\n",
        "record_func(3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}